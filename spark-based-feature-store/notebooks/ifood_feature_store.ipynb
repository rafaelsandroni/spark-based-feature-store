{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ifood Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "- Uma abordagem de feature store usando Pyspark, uma estrutura simples, flexível e confiável.\n",
    "- Pelo tempo de desenvolvimento limitado, optei por não inventar a roda, e utilizar um framework que abstrai boa parte dos componentes do Pyspark, como integração com kafka, transformações de DataFrames, e armazenamento dos dados. O butterfree é um framework open source e brasileiro, desenvolvido pelo QuintoAndar, e atende bem quando o escopo é de tamanho pequeno ou médio.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import session, SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "# butterfree spark client\n",
    "from butterfree.clients import SparkClient\n",
    "import os\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "from butterfree.extract import Source\n",
    "from butterfree.extract.readers import FileReader\n",
    "from butterfree.extract.readers import KafkaReader\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField, DoubleType, BinaryType, TimestampType, DateType\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from butterfree.transform import FeatureSet\n",
    "from butterfree.transform.features import Feature, KeyFeature, TimestampFeature\n",
    "from butterfree.transform.transformations import SQLExpressionTransform, SparkFunctionTransform, CustomTransform\n",
    "from butterfree.transform.transformations.h3_transform import H3HashTransform\n",
    "from butterfree.constants import DataType\n",
    "from butterfree.transform.utils import Function\n",
    "\n",
    "from butterfree.load.writers import (\n",
    "    HistoricalFeatureStoreWriter,\n",
    "    OnlineFeatureStoreWriter,\n",
    ")\n",
    "from butterfree.load import Sink\n",
    "\n",
    "from butterfree.configs.db import CassandraConfig\n",
    "from butterfree.load.writers import OnlineFeatureStoreWriter\n",
    "from butterfree.pipelines import FeatureSetPipeline\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[6] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0-beta pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    \n",
    "    hive_metastore = \"thrift://hive-metastore:9083\"\n",
    "\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Feature Store\")\n",
    "        .config(\"spark.sql.warehouse.dir\", hive_metastore)\n",
    "        .config(\"spark.hive.metastore.uris\", hive_metastore)\n",
    "        .config(\"spark.executor.memory\", \"8g\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    spark_client = SparkClient()\n",
    "    hive_context = HiveContext(sc)\n",
    "\n",
    "    return spark_client, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_client, spark = spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Kafka source and they topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = \"a49784be7f36511e9a6b60a341003dc2-1378330561.us-east-1.elb.amazonaws.com:9092\"\n",
    "topic = \"de-order-events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'de-order-events', 'testTopic', 'queueing.transactions', 'de-order-aaa', 'order', 'de-order-status-events', 'numtest', 'client', 'my-topic', 'de-restaurant-events', 'de-consumer-events'}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "nbrrecords = int(50)\n",
    "nbrrecordsinserted = int(0)\n",
    "nbrrecordsretreived = int(0)\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=client, group_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client',\n",
       " 'de-consumer-events',\n",
       " 'de-order-aaa',\n",
       " 'de-order-events',\n",
       " 'de-order-status-events',\n",
       " 'de-restaurant-events',\n",
       " 'my-topic',\n",
       " 'numtest',\n",
       " 'order',\n",
       " 'queueing.transactions',\n",
       " 'testTopic'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-restaurant-events\n",
      "de-restaurant-events 7292\n",
      "de-order-events\n",
      "de-order-events 3683040\n",
      "de-consumer-events\n",
      "de-consumer-events 809323\n"
     ]
    }
   ],
   "source": [
    "topics = ['de-restaurant-events','de-order-events','de-consumer-events']\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    tp = TopicPartition(topic,0)\n",
    "    consumer.assign([tp])\n",
    "    consumer.seek_to_beginning(tp)\n",
    "    # obtain the last offset value\n",
    "    lastOffset = consumer.end_offsets([tp])[tp]\n",
    "    print(topic, lastOffset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to be considered\n",
    "\n",
    "### orders\n",
    "selected columns:\n",
    "\n",
    "cpf,\n",
    "customer_id,\n",
    "items,\n",
    "order_total_amount,\n",
    "order_created_at,\n",
    "order_scheduled,\n",
    "delivery_address_latitude,\n",
    "delivery_address_longitude,\n",
    "\n",
    "### restaurants\n",
    "selected columns:\n",
    "\n",
    "average_ticket,\n",
    "takeout_time,\n",
    "delivery_time,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting kafka readers (using native spark stream kafka reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TimestampType, DateType\n",
    "\n",
    "orders_schema = StructType([\n",
    "                        StructField('cpf', IntegerType(), True),\n",
    "                        StructField('customer_id', StringType(), True),\n",
    "                        StructField('merchant_id', StringType(), True),\n",
    "                        StructField('items', StringType(), True),\n",
    "                        StructField('order_total_amount', DoubleType(), True),\n",
    "                        StructField('delivery_address_latitude', DoubleType(), True),\n",
    "                        StructField('delivery_address_longitude', DoubleType(), True),\n",
    "                        StructField('order_scheduled', IntegerType(), True),\n",
    "                        StructField('order_created_at', TimestampType(), True)\n",
    "                ])\n",
    "\n",
    "restaurants_schema = StructType([\n",
    "    \n",
    "                        StructField('id', StringType(), True),\n",
    "                        StructField('average_ticket', DoubleType(), True),\n",
    "                        StructField('takeout_time', IntegerType(), True),\n",
    "                        StructField('delivery_time', IntegerType(), True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_topic = \"de-order-events\"\n",
    "restaurants_topic = \"de-restaurant-events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I've setted startingOffset to run entire pipeline more fastly\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "\n",
    "kafka_orders_reader = KafkaReader(\n",
    "    id=\"order_events\",\n",
    "    topic=orders_topic,\n",
    "    value_schema=orders_schema,\n",
    "    connection_string=client,\n",
    "    topic_options={\"startingOffsets\": \"\"\" {\"de-order-events\": {\"0\":3682000}} \"\"\"},\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "kafka_restaurants_reader = KafkaReader(\n",
    "    id=\"restaurants_events\",\n",
    "    topic=restaurants_topic,\n",
    "    value_schema=restaurants_schema,\n",
    "    connection_string=client,\n",
    "    topic_options={\"startingOffsets\": \"\"\" {\"de-restaurant-events\": {\"0\":7200}} \"\"\"},\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "readers = [\n",
    "    kafka_orders_reader,\n",
    "    kafka_restaurants_reader\n",
    "    \n",
    "]\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    *\n",
    "from\n",
    "    order_events\n",
    "    join restaurants_events\n",
    "        on order_events.merchant_id = restaurants_events.id    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "source = Source(readers=readers, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<butterfree.extract.source.Source at 0x7ff73ed0d040>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+------------------+-------------------------+--------------------------+---------------+-------------------+--------------------+--------------------+--------------+------------+-------------+--------------------+\n",
      "| cpf|         customer_id|         merchant_id|               items|order_total_amount|delivery_address_latitude|delivery_address_longitude|order_scheduled|   order_created_at|      kafka_metadata|                  id|average_ticket|takeout_time|delivery_time|      kafka_metadata|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------------------+--------------------------+---------------+-------------------+--------------------+--------------------+--------------+------------+-------------+--------------------+\n",
      "|null|0ddebc9b-39c1-4ea...|10554b0b-8a67-454...|[{\"name\": \"Guioza...|              51.8|                   -47.92|                    -15.84|           null|2019-01-12 23:31:47|[, de-order-event...|10554b0b-8a67-454...|          30.0|           0|           90|[, de-restaurant-...|\n",
      "|null|3590abad-efb5-462...|24409a29-3a8e-4c7...|[{\"name\": \"Sexta ...|              14.9|                   -38.49|                     -3.74|           null|2019-01-08 22:08:14|[, de-order-event...|24409a29-3a8e-4c7...|          40.0|           0|            0|[, de-restaurant-...|\n",
      "|null|af30d521-5a68-4ca...|3b10106d-f9ab-447...|[{\"name\": \"MIX YA...|              54.4|                   -46.71|                    -23.58|           null|2019-01-30 17:20:51|[, de-order-event...|3b10106d-f9ab-447...|          60.0|           0|           35|[, de-restaurant-...|\n",
      "|null|c0844066-28e1-465...|1eab01b5-ae23-427...|[{\"name\": \"BROTO\"...|              21.0|                   -46.67|                    -23.68|           null|2019-01-20 00:19:09|[, de-order-event...|1eab01b5-ae23-427...|          30.0|           0|           45|[, de-restaurant-...|\n",
      "|null|b38b218e-3e22-44f...|e55618b4-9bfb-4c2...|[{\"name\": \"Torta ...|              46.0|                   -46.47|                    -23.53|           null|2019-01-08 16:30:26|[, de-order-event...|e55618b4-9bfb-4c2...|          40.0|           0|            0|[, de-restaurant-...|\n",
      "|null|79677fb6-31c7-4dd...|9941f520-8dab-482...|[{\"name\": \"Filet ...|              65.3|                   -46.65|                    -23.62|           null|2018-12-29 16:16:57|[, de-order-event...|9941f520-8dab-482...|          80.0|           0|            0|[, de-restaurant-...|\n",
      "|null|373e5d95-a3bd-484...|f994f21e-8608-46e...|[{\"name\": \"GRANDE...|              44.0|                    -49.3|                    -25.43|           null|2019-01-18 00:17:13|[, de-order-event...|f994f21e-8608-46e...|          60.0|           0|           40|[, de-restaurant-...|\n",
      "|null|6065622a-98db-4a6...|19894810-7994-411...|[{\"name\": \"CHEDDA...|              45.8|                    -38.5|                     -3.74|           null|2019-01-31 00:01:05|[, de-order-event...|19894810-7994-411...|          60.0|           0|            0|[, de-restaurant-...|\n",
      "|null|1da1119e-cf94-47b...|19894810-7994-411...|[{\"name\": \"COCA C...|             168.3|                   -38.49|                     -3.75|           null|2019-01-08 21:04:42|[, de-order-event...|19894810-7994-411...|          60.0|           0|            0|[, de-restaurant-...|\n",
      "|null|fa5789d2-2ff9-4b6...|9738e63c-6882-47d...|[{\"name\": \"Lambar...|              73.5|                   -49.24|                    -16.71|           null|2019-01-29 20:55:22|[, de-order-event...|9738e63c-6882-47d...|          60.0|           0|           45|[, de-restaurant-...|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------------------+--------------------------+---------------+-------------------+--------------------+--------------------+--------------+------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df = source.construct(spark_client)\n",
    "\n",
    "source_df = source_df.limit(40)\n",
    "\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature, custom transformations\n",
    "\n",
    "days = lambda i: i * 86400 # convert days to seconds\n",
    "\n",
    "def count_items(df, parent_feature, column):\n",
    "    \n",
    "    name = parent_feature.get_output_columns()[0]\n",
    "    \n",
    "    #df = df.withColumn(name, F.lit(10))\n",
    "    df = df.withColumn(name, F.size(F.split(F.col(column), r\"name\")) - 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def avg_last_1_month(df, parent_feature, column):\n",
    "    \n",
    "    name = parent_feature.get_output_columns()[0]\n",
    "    \n",
    "    windowSpec = W.partitionBy(\"customer_id\").orderBy(F.col(\"order_created_at\").cast('long')).rangeBetween(-days(30), 0)\n",
    "    \n",
    "    df = df.withColumn(name, F.avg(column).over(windowSpec))\n",
    "\n",
    "    return df\n",
    "\n",
    "def divide(df, fs, column1, column2):\n",
    "    name = fs.get_output_columns()[0]\n",
    "    df = df.withColumn(name, F.col(column1) / F.col(column2))\n",
    "    return df\n",
    "\n",
    "def count_items_in_order():\n",
    "    return Feature(\n",
    "        name=\"items_qtd\",\n",
    "        description=\"count number of items in order\",\n",
    "        dtype=DataType.INTEGER,\n",
    "        transformation=CustomTransform(\n",
    "           transformer=count_items, column=\"items\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def avg_order_total_amount_from_last_1_month():\n",
    "    return Feature(\n",
    "        name=\"avg_order_amount_from_last_1_month_val\",\n",
    "        description=\"average order amount from last 1 month\",\n",
    "        dtype=DataType.DOUBLE,\n",
    "        transformation=CustomTransform(\n",
    "           transformer=avg_last_1_month, column=\"order_total_amount\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def order_total_amount():\n",
    "    return Feature(\n",
    "        name=\"order_total_amount\",\n",
    "        description=\"name_employer\",\n",
    "        dtype=DataType.STRING,\n",
    "    )\n",
    "\n",
    "def ratio_order_amount_and_items():\n",
    "    return Feature(\n",
    "        name=\"ratio_order_amount_by_items_val\",\n",
    "        description=\"ratio order amount by items count\",\n",
    "        dtype=DataType.DOUBLE,\n",
    "        transformation=CustomTransform(\n",
    "           transformer=divide, column1=\"order_total_amount\", column2=\"items_qtd\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def ratio_order_amount_and_average_ticket():\n",
    "    return Feature(\n",
    "        name=\"ratio_order_amount_by_average_ticket_val\",\n",
    "        description=\"ratio order amount by restaurant average ticket\",\n",
    "        dtype=DataType.DOUBLE,\n",
    "        transformation=CustomTransform(\n",
    "           transformer=divide, column1=\"order_total_amount\", column2=\"average_ticket\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding transformations into a features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary key\n",
    "keys = [\n",
    "    KeyFeature(\n",
    "        name=\"customer_id\",\n",
    "        description=\"Unique identificator code for customer.\",\n",
    "        from_column=\"customer_id\",\n",
    "        dtype=DataType.STRING,\n",
    "    )\n",
    "]\n",
    "\n",
    "ts_feature = TimestampFeature(from_column=\"order_created_at\")\n",
    "\n",
    "# features transformations\n",
    "features = [\n",
    "    #order_total_amount(),\n",
    "    count_items_in_order(),\n",
    "    avg_order_total_amount_from_last_1_month(),\n",
    "    ratio_order_amount_and_items(),\n",
    "    ratio_order_amount_and_average_ticket()\n",
    "]\n",
    "\n",
    "# joining all together\n",
    "feature_set = FeatureSet(\n",
    "    name=\"orders_feature_master_table\",\n",
    "    entity=\"orders_feature_master_table\",  # entity: to which \"business context\" this feature set belongs\n",
    "    description=\"Features describring events about ifood store.\",\n",
    "    keys=keys,\n",
    "    timestamp=ts_feature,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/butterfree/transform/features/feature.py:117: UserWarning: The column name customer_id already exists in the dataframe and will be overwritten with another column.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+----------------------------+-------------------------------+----------------------------------------+\n",
      "|         customer_id|          timestamp|items_count_val|avg_order_amount_1_month_val|ratio_order_amount_by_items_val|ratio_order_amount_by_average_ticket_val|\n",
      "+--------------------+-------------------+---------------+----------------------------+-------------------------------+----------------------------------------+\n",
      "|0ddebc9b-39c1-4ea...|2019-01-12 23:31:47|              2|                        51.8|                           25.9|                      1.7266666666666666|\n",
      "|1da1119e-cf94-47b...|2019-01-08 21:04:42|              2|                       168.3|                          84.15|                                   2.805|\n",
      "|3590abad-efb5-462...|2019-01-08 22:08:14|              1|                        14.9|                           14.9|                                  0.3725|\n",
      "|373e5d95-a3bd-484...|2019-01-18 00:17:13|              6|                        44.0|              7.333333333333333|                      0.7333333333333333|\n",
      "|6065622a-98db-4a6...|2019-01-31 00:01:05|              2|                        45.8|                           22.9|                      0.7633333333333333|\n",
      "|79677fb6-31c7-4dd...|2018-12-29 16:16:57|              3|                        65.3|             21.766666666666666|                      0.8162499999999999|\n",
      "|af30d521-5a68-4ca...|2019-01-30 17:20:51|              6|                        54.4|              9.066666666666666|                      0.9066666666666666|\n",
      "|b38b218e-3e22-44f...|2019-01-08 16:30:26|              6|                        46.0|              7.666666666666667|                                    1.15|\n",
      "|c0844066-28e1-465...|2019-01-20 00:19:09|              5|                        21.0|                            4.2|                                     0.7|\n",
      "|fa5789d2-2ff9-4b6...|2019-01-29 20:55:22|              4|                        73.5|                         18.375|                                   1.225|\n",
      "+--------------------+-------------------+---------------+----------------------------+-------------------------------+----------------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_set_df = feature_set.construct(source_df, spark_client)\n",
    "\n",
    "print(feature_set_df.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cluster = Cluster(['feature_store_cassandra'])\n",
    "session = cluster.connect()\n",
    "df = session.execute(\"DROP TABLE feature_store.orders_feature_master_table\")\n",
    "cluster.shutdown()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cassandra DB for online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _cassandra_config():\n",
    "    return CassandraConfig(\n",
    "        username=\"cassandra\", \n",
    "        password=\"cassandra\",\n",
    "        host=\"feature_store_cassandra\",\n",
    "        keyspace=\"feature_store\",\n",
    "        stream_checkpoint_path=\"./\"\n",
    "    )\n",
    "\n",
    "def _create_table(feature_set_df):\n",
    "\n",
    "    keyspace = \"feature_store\"\n",
    "    table_name = \"orders_feature_master_table\"\n",
    "\n",
    "    cassandra_mapping = {\n",
    "            \"TimestampType\": \"timestamp\",\n",
    "            \"BinaryType\": \"boolean\",\n",
    "            \"BooleanType\": \"boolean\",\n",
    "            \"DateType\": \"timestamp\",\n",
    "            \"DecimalType\": \"decimal\",\n",
    "            \"DoubleType\": \"double\",\n",
    "            \"FloatType\": \"float\",\n",
    "            \"IntegerType\": \"int\",\n",
    "            \"LongType\": \"bigint\",\n",
    "            \"StringType\": \"text\",\n",
    "            \"ArrayType(LongType,true)\": \"frozen<list<bigint>>\",\n",
    "            \"ArrayType(StringType,true)\": \"frozen<list<text>>\",\n",
    "            \"ArrayType(FloatType,true)\": \"frozen<list<float>>\",\n",
    "        }\n",
    "\n",
    "    cluster = Cluster(['feature_store_cassandra'])\n",
    "    session = cluster.connect()\n",
    "\n",
    "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS \"+ keyspace +\" WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1 };\")\n",
    "\n",
    "    sql = \", \".join([feature.name +str(\" \") + cassandra_mapping[str(feature.dataType)] for feature in feature_set_df.schema]).replace(\"customer_id text\", \"customer_id text PRIMARY KEY\")\n",
    "    \n",
    "    sql = \"CREATE TABLE IF NOT EXISTS {}.{} (\" + sql + \");\"\n",
    "    sql = sql.format(keyspace, table_name)\n",
    "    session.execute(sql)\n",
    "    cluster.shutdown()\n",
    "\n",
    "db_config = _cassandra_config()\n",
    "\n",
    "_create_table(feature_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting writes (Metastore and Cassandra) and combining into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers = [HistoricalFeatureStoreWriter(debug_mode=True), OnlineFeatureStoreWriter(db_config=db_config)]\n",
    "\n",
    "#writers = [HistoricalFeatureStoreWriter(debug_mode=True)]\n",
    "\n",
    "sink = Sink(writers=writers)\n",
    "\n",
    "pipeline = FeatureSetPipeline(source=source, feature_set=feature_set, sink=sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pipeline end2end\n",
    "They will run until data from queue are finished.\n",
    "\n",
    "All data will be stored into historical features, otherwise, for online features, aways will be updated with most recent data, considering consumer_id (primary key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consuming data from feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online features, from Cassandra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>avg_order_amount_1_month_val</th>\n",
       "      <th>items_count_val</th>\n",
       "      <th>ratio_order_amount_by_average_ticket_val</th>\n",
       "      <th>ratio_order_amount_by_items_val</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0844066-28e1-4658-b375-91fa8173c7e2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>2019-01-20 00:19:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b38b218e-3e22-44f2-adea-fd2c9dcfb431</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>2019-01-08 16:30:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6065622a-98db-4a65-93a1-ba89e9f7ab7d</td>\n",
       "      <td>45.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>2019-01-31 00:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af30d521-5a68-4ca7-8d66-7bd8e03d7bda</td>\n",
       "      <td>54.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>2019-01-30 17:20:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79677fb6-31c7-4ddc-b35c-2afe15d1f96b</td>\n",
       "      <td>65.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.816250</td>\n",
       "      <td>21.766667</td>\n",
       "      <td>2018-12-29 16:16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3590abad-efb5-4622-a98c-ed70856006a7</td>\n",
       "      <td>14.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>2019-01-08 22:08:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>373e5d95-a3bd-484e-927e-ac4c3bdbe1c6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>2019-01-18 00:17:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1da1119e-cf94-47bf-ae73-3b4f9d7b7196</td>\n",
       "      <td>168.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.805000</td>\n",
       "      <td>84.150000</td>\n",
       "      <td>2019-01-08 21:04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fa5789d2-2ff9-4b62-8bd4-41daac2bec63</td>\n",
       "      <td>73.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>18.375000</td>\n",
       "      <td>2019-01-29 20:55:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0ddebc9b-39c1-4ea0-ad4f-6fe68e7b26ec</td>\n",
       "      <td>51.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.726667</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>2019-01-12 23:31:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id  avg_order_amount_1_month_val  \\\n",
       "0  c0844066-28e1-4658-b375-91fa8173c7e2                          21.0   \n",
       "1  b38b218e-3e22-44f2-adea-fd2c9dcfb431                          46.0   \n",
       "2  6065622a-98db-4a65-93a1-ba89e9f7ab7d                          45.8   \n",
       "3  af30d521-5a68-4ca7-8d66-7bd8e03d7bda                          54.4   \n",
       "4  79677fb6-31c7-4ddc-b35c-2afe15d1f96b                          65.3   \n",
       "5  3590abad-efb5-4622-a98c-ed70856006a7                          14.9   \n",
       "6  373e5d95-a3bd-484e-927e-ac4c3bdbe1c6                          44.0   \n",
       "7  1da1119e-cf94-47bf-ae73-3b4f9d7b7196                         168.3   \n",
       "8  fa5789d2-2ff9-4b62-8bd4-41daac2bec63                          73.5   \n",
       "9  0ddebc9b-39c1-4ea0-ad4f-6fe68e7b26ec                          51.8   \n",
       "\n",
       "   items_count_val  ratio_order_amount_by_average_ticket_val  \\\n",
       "0                5                                  0.700000   \n",
       "1                6                                  1.150000   \n",
       "2                2                                  0.763333   \n",
       "3                6                                  0.906667   \n",
       "4                3                                  0.816250   \n",
       "5                1                                  0.372500   \n",
       "6                6                                  0.733333   \n",
       "7                2                                  2.805000   \n",
       "8                4                                  1.225000   \n",
       "9                2                                  1.726667   \n",
       "\n",
       "   ratio_order_amount_by_items_val           timestamp  \n",
       "0                         4.200000 2019-01-20 00:19:09  \n",
       "1                         7.666667 2019-01-08 16:30:26  \n",
       "2                        22.900000 2019-01-31 00:01:05  \n",
       "3                         9.066667 2019-01-30 17:20:51  \n",
       "4                        21.766667 2018-12-29 16:16:57  \n",
       "5                        14.900000 2019-01-08 22:08:14  \n",
       "6                         7.333333 2019-01-18 00:17:13  \n",
       "7                        84.150000 2019-01-08 21:04:42  \n",
       "8                        18.375000 2019-01-29 20:55:22  \n",
       "9                        25.900000 2019-01-12 23:31:47  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['feature_store_cassandra'])\n",
    "session = cluster.connect()\n",
    "df = session.execute(\"SELECT * FROM feature_store.orders_feature_master_table\")\n",
    "cluster.shutdown()\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(df)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical features, from Hive Metastore \n",
    "Here I'm using PostgreSQL as storage, but it can be switched to S2 easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------+-----------+\n",
      "|database|tableName                                            |isTemporary|\n",
      "+--------+-----------------------------------------------------+-----------+\n",
      "|        |historical_feature_store__orders_feature_master_table|true       |\n",
      "|        |order_events                                         |true       |\n",
      "|        |restaurants_events                                   |true       |\n",
      "+--------+-----------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>items_count_val</th>\n",
       "      <th>avg_order_amount_1_month_val</th>\n",
       "      <th>ratio_order_amount_by_items_val</th>\n",
       "      <th>ratio_order_amount_by_average_ticket_val</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0844066-28e1-4658-b375-91fa8173c7e2</td>\n",
       "      <td>2019-01-20 00:19:09</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6065622a-98db-4a65-93a1-ba89e9f7ab7d</td>\n",
       "      <td>2019-01-31 00:01:05</td>\n",
       "      <td>2</td>\n",
       "      <td>45.8</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0ddebc9b-39c1-4ea0-ad4f-6fe68e7b26ec</td>\n",
       "      <td>2019-01-12 23:31:47</td>\n",
       "      <td>2</td>\n",
       "      <td>51.8</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>1.726667</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79677fb6-31c7-4ddc-b35c-2afe15d1f96b</td>\n",
       "      <td>2018-12-29 16:16:57</td>\n",
       "      <td>3</td>\n",
       "      <td>65.3</td>\n",
       "      <td>21.766667</td>\n",
       "      <td>0.816250</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af30d521-5a68-4ca7-8d66-7bd8e03d7bda</td>\n",
       "      <td>2019-01-30 17:20:51</td>\n",
       "      <td>6</td>\n",
       "      <td>54.4</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>373e5d95-a3bd-484e-927e-ac4c3bdbe1c6</td>\n",
       "      <td>2019-01-18 00:17:13</td>\n",
       "      <td>6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fa5789d2-2ff9-4b62-8bd4-41daac2bec63</td>\n",
       "      <td>2019-01-29 20:55:22</td>\n",
       "      <td>4</td>\n",
       "      <td>73.5</td>\n",
       "      <td>18.375000</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1da1119e-cf94-47bf-ae73-3b4f9d7b7196</td>\n",
       "      <td>2019-01-08 21:04:42</td>\n",
       "      <td>2</td>\n",
       "      <td>168.3</td>\n",
       "      <td>84.150000</td>\n",
       "      <td>2.805000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3590abad-efb5-4622-a98c-ed70856006a7</td>\n",
       "      <td>2019-01-08 22:08:14</td>\n",
       "      <td>1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b38b218e-3e22-44f2-adea-fd2c9dcfb431</td>\n",
       "      <td>2019-01-08 16:30:26</td>\n",
       "      <td>6</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id           timestamp  items_count_val  \\\n",
       "0  c0844066-28e1-4658-b375-91fa8173c7e2 2019-01-20 00:19:09                5   \n",
       "1  6065622a-98db-4a65-93a1-ba89e9f7ab7d 2019-01-31 00:01:05                2   \n",
       "2  0ddebc9b-39c1-4ea0-ad4f-6fe68e7b26ec 2019-01-12 23:31:47                2   \n",
       "3  79677fb6-31c7-4ddc-b35c-2afe15d1f96b 2018-12-29 16:16:57                3   \n",
       "4  af30d521-5a68-4ca7-8d66-7bd8e03d7bda 2019-01-30 17:20:51                6   \n",
       "5  373e5d95-a3bd-484e-927e-ac4c3bdbe1c6 2019-01-18 00:17:13                6   \n",
       "6  fa5789d2-2ff9-4b62-8bd4-41daac2bec63 2019-01-29 20:55:22                4   \n",
       "7  1da1119e-cf94-47bf-ae73-3b4f9d7b7196 2019-01-08 21:04:42                2   \n",
       "8  3590abad-efb5-4622-a98c-ed70856006a7 2019-01-08 22:08:14                1   \n",
       "9  b38b218e-3e22-44f2-adea-fd2c9dcfb431 2019-01-08 16:30:26                6   \n",
       "\n",
       "   avg_order_amount_1_month_val  ratio_order_amount_by_items_val  \\\n",
       "0                          21.0                         4.200000   \n",
       "1                          45.8                        22.900000   \n",
       "2                          51.8                        25.900000   \n",
       "3                          65.3                        21.766667   \n",
       "4                          54.4                         9.066667   \n",
       "5                          44.0                         7.333333   \n",
       "6                          73.5                        18.375000   \n",
       "7                         168.3                        84.150000   \n",
       "8                          14.9                        14.900000   \n",
       "9                          46.0                         7.666667   \n",
       "\n",
       "   ratio_order_amount_by_average_ticket_val  year  month  day  \n",
       "0                                  0.700000  2019      1   20  \n",
       "1                                  0.763333  2019      1   31  \n",
       "2                                  1.726667  2019      1   12  \n",
       "3                                  0.816250  2018     12   29  \n",
       "4                                  0.906667  2019      1   30  \n",
       "5                                  0.733333  2019      1   18  \n",
       "6                                  1.225000  2019      1   29  \n",
       "7                                  2.805000  2019      1    8  \n",
       "8                                  0.372500  2019      1    8  \n",
       "9                                  1.150000  2019      1    8  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"historical_feature_store__orders_feature_master_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
