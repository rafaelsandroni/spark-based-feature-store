Found orphan containers (pyspark-jupyter) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.
Starting spark-based-feature-store_hive-metastore-postgresql_1 ... 
Starting namenode                                              ... 
Starting datanode                                              ... 
Starting feature_store_cassandra                               ... 
Starting zookeeper                                             ... 
Starting hive-metastore                                        ... 
Starting pyspark-app                                           ... 
Starting hive-server                                           ... 
[8A[2KStarting spark-based-feature-store_hive-metastore-postgresql_1 ... [32mdone[0m[8B[6A[2KStarting datanode                                              ... [32mdone[0m[6B[7A[2KStarting namenode                                              ... [32mdone[0m[7B[4A[2KStarting zookeeper                                             ... [32mdone[0m[4BStarting broker                                                ... 
[6A[2KStarting feature_store_cassandra                               ... [32mdone[0m[6B[4A[2KStarting hive-metastore                                        ... [32mdone[0m[4B[3A[2KStarting pyspark-app                                           ... [32mdone[0m[3B[2A[2KStarting hive-server                                           ... [32mdone[0m[2B[1A[2KStarting broker                                                ... [32mdone[0m[1BAttaching to spark-based-feature-store_hive-metastore-postgresql_1, datanode, namenode, zookeeper, feature_store_cassandra, hive-metastore, pyspark-app, hive-server, broker
[33mdatanode                     |[0m Configuring core
[33mdatanode                     |[0m  - Setting hadoop.proxyuser.hue.hosts=*
[33mdatanode                     |[0m  - Setting fs.defaultFS=hdfs://namenode:8020
[33mdatanode                     |[0m  - Setting hadoop.proxyuser.hue.groups=*
[33mdatanode                     |[0m  - Setting hadoop.http.staticuser.user=root
[33mdatanode                     |[0m Configuring hdfs
[33mdatanode                     |[0m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false
[33mdatanode                     |[0m  - Setting dfs.datanode.data.dir=file:///hadoop/dfs/data
[33mdatanode                     |[0m  - Setting dfs.permissions.enabled=false
[33mdatanode                     |[0m  - Setting dfs.webhdfs.enabled=true
[33mdatanode                     |[0m Configuring yarn
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate
[33mdatanode                     |[0m  - Setting yarn.timeline-service.generic-application-history.enabled=true
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.recovery.enabled=true
[33mdatanode                     |[0m  - Setting yarn.timeline-service.enabled=true
[33mdatanode                     |[0m  - Setting yarn.log-aggregation-enable=true
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true
[33mdatanode                     |[0m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.resource.tracker.address=resourcemanager:8031
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.hostname=resourcemanager
[33mdatanode                     |[0m  - Setting yarn.timeline-service.hostname=historyserver
[33mdatanode                     |[0m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030
[33mdatanode                     |[0m  - Setting yarn.resourcemanager.address=resourcemanager:8032
[33mdatanode                     |[0m Configuring httpfs
[33mdatanode                     |[0m Configuring kms
[33mdatanode                     |[0m Configuring mapred
[33mdatanode                     |[0m Configuring for multihomed network
[33mdatanode                     |[0m [1/100] check for namenode:50070...
[33mdatanode                     |[0m [1/100] namenode:50070 is not available yet
[33mdatanode                     |[0m [1/100] try in 5s once again ...
[36mbroker                       |[0m ===> User
[36mbroker                       |[0m uid=0(root) gid=0(root) groups=0(root)
[36mbroker                       |[0m ===> Configuring ...
[35mhive-metastore               |[0m Configuring core
[35mhive-metastore               |[0m  - Setting hadoop.proxyuser.hue.hosts=*
[35mhive-metastore               |[0m  - Setting fs.defaultFS=hdfs://namenode:8020
[35mhive-metastore               |[0m  - Setting hadoop.proxyuser.hue.groups=*
[35mhive-metastore               |[0m  - Setting hadoop.http.staticuser.user=root
[35mhive-metastore               |[0m Configuring hdfs
[35mhive-metastore               |[0m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false
[35mhive-metastore               |[0m  - Setting dfs.permissions.enabled=false
[35mhive-metastore               |[0m  - Setting dfs.webhdfs.enabled=true
[35mhive-metastore               |[0m Configuring yarn
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate
[35mhive-metastore               |[0m  - Setting yarn.timeline-service.generic-application-history.enabled=true
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.recovery.enabled=true
[35mhive-metastore               |[0m  - Setting yarn.timeline-service.enabled=true
[35mhive-metastore               |[0m  - Setting yarn.log-aggregation-enable=true
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true
[35mhive-metastore               |[0m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.resource.tracker.address=resourcemanager:8031
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.hostname=resourcemanager
[34mhive-server                  |[0m Configuring core
[34mhive-server                  |[0m  - Setting hadoop.proxyuser.hue.hosts=*
[34mhive-server                  |[0m  - Setting fs.defaultFS=hdfs://namenode:8020
[34mhive-server                  |[0m  - Setting hadoop.proxyuser.hue.groups=*
[34mhive-server                  |[0m  - Setting hadoop.http.staticuser.user=root
[34mhive-server                  |[0m Configuring hdfs
[34mhive-server                  |[0m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false
[34mhive-server                  |[0m  - Setting dfs.permissions.enabled=false
[34mhive-server                  |[0m  - Setting dfs.webhdfs.enabled=true
[34mhive-server                  |[0m Configuring yarn
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate
[34mhive-server                  |[0m  - Setting yarn.timeline-service.generic-application-history.enabled=true
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.recovery.enabled=true
[34mhive-server                  |[0m  - Setting yarn.timeline-service.enabled=true
[34mhive-server                  |[0m  - Setting yarn.log-aggregation-enable=true
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true
[34mhive-server                  |[0m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.resource.tracker.address=resourcemanager:8031
[35mhive-metastore               |[0m  - Setting yarn.timeline-service.hostname=historyserver
[35mhive-metastore               |[0m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.hostname=resourcemanager
[34mhive-server                  |[0m  - Setting yarn.timeline-service.hostname=historyserver
[34mhive-server                  |[0m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030
[34mhive-server                  |[0m  - Setting yarn.resourcemanager.address=resourcemanager:8032
[34mhive-server                  |[0m Configuring httpfs
[34mhive-server                  |[0m Configuring kms
[35mhive-metastore               |[0m  - Setting yarn.resourcemanager.address=resourcemanager:8032
[36;1mnamenode                     |[0m Configuring core
[36;1mnamenode                     |[0m  - Setting hadoop.proxyuser.hue.hosts=*
[36;1mnamenode                     |[0m  - Setting fs.defaultFS=hdfs://namenode:8020
[36;1mnamenode                     |[0m  - Setting hadoop.proxyuser.hue.groups=*
[36;1mnamenode                     |[0m  - Setting hadoop.http.staticuser.user=root
[36;1mnamenode                     |[0m Configuring hdfs
[36;1mnamenode                     |[0m  - Setting dfs.namenode.name.dir=file:///hadoop/dfs/name
[36;1mnamenode                     |[0m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false
[36;1mnamenode                     |[0m  - Setting dfs.permissions.enabled=false
[36;1mnamenode                     |[0m  - Setting dfs.webhdfs.enabled=true
[36;1mnamenode                     |[0m Configuring yarn
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate
[36;1mnamenode                     |[0m  - Setting yarn.timeline-service.generic-application-history.enabled=true
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.recovery.enabled=true
[36;1mnamenode                     |[0m  - Setting yarn.timeline-service.enabled=true
[36;1mnamenode                     |[0m  - Setting yarn.log-aggregation-enable=true
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true
[36;1mnamenode                     |[0m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.resource.tracker.address=resourcemanager:8031
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.hostname=resourcemanager
[36;1mnamenode                     |[0m  - Setting yarn.timeline-service.hostname=historyserver
[36;1mnamenode                     |[0m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030
[36;1mnamenode                     |[0m  - Setting yarn.resourcemanager.address=resourcemanager:8032
[36;1mnamenode                     |[0m Configuring httpfs
[36;1mnamenode                     |[0m Configuring kms
[36;1mnamenode                     |[0m Configuring mapred
[36;1mnamenode                     |[0m Configuring for multihomed network
[34mhive-server                  |[0m Configuring mapred
[35mhive-metastore               |[0m Configuring httpfs
[35mhive-metastore               |[0m Configuring kms
[35mhive-metastore               |[0m Configuring mapred
[35mhive-metastore               |[0m Configuring hive
[35mhive-metastore               |[0m  - Setting hive.metastore.uris=thrift://hive-metastore:9083
[35mhive-metastore               |[0m  - Setting datanucleus.autoCreateSchema=false
[35mhive-metastore               |[0m  - Setting javax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-postgresql/metastore
[35mhive-metastore               |[0m  - Setting javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
[35mhive-metastore               |[0m  - Setting javax.jdo.option.ConnectionPassword=hive
[35mhive-metastore               |[0m  - Setting javax.jdo.option.ConnectionUserName=hive
[35mhive-metastore               |[0m Configuring for multihomed network
[32;1mhive-metastore-postgresql_1  |[0m LOG:  database system was shut down at 2020-12-04 14:22:28 UTC
[32;1mhive-metastore-postgresql_1  |[0m LOG:  MultiXact member wraparound protections are now enabled
[32;1mhive-metastore-postgresql_1  |[0m LOG:  database system is ready to accept connections
[32;1mhive-metastore-postgresql_1  |[0m LOG:  autovacuum launcher started
[35;1mzookeeper                    |[0m ===> User
[35;1mzookeeper                    |[0m uid=0(root) gid=0(root) groups=0(root)
[35;1mzookeeper                    |[0m ===> Configuring ...
[34mhive-server                  |[0m Configuring hive
[34mhive-server                  |[0m  - Setting hive.metastore.uris=thrift://hive-metastore:9083
[34mhive-server                  |[0m  - Setting datanucleus.autoCreateSchema=false
[34mhive-server                  |[0m  - Setting javax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-postgresql/metastore
[34mhive-server                  |[0m  - Setting javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
[34mhive-server                  |[0m  - Setting javax.jdo.option.ConnectionPassword=hive
[35mhive-metastore               |[0m [1/100] check for namenode:50070...
[35mhive-metastore               |[0m [1/100] namenode:50070 is not available yet
[35mhive-metastore               |[0m [1/100] try in 5s once again ...
[34mhive-server                  |[0m  - Setting javax.jdo.option.ConnectionUserName=hive
[34mhive-server                  |[0m Configuring for multihomed network
[34mhive-server                  |[0m [1/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [1/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [1/100] try in 5s once again ...
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO namenode.NameNode: STARTUP_MSG: 
[36;1mnamenode                     |[0m /************************************************************
[36;1mnamenode                     |[0m STARTUP_MSG: Starting NameNode
[36;1mnamenode                     |[0m STARTUP_MSG:   host = 7b64f85afdef/172.21.0.4
[36;1mnamenode                     |[0m STARTUP_MSG:   args = []
[36;1mnamenode                     |[0m STARTUP_MSG:   version = 2.7.4
[36;1mnamenode                     |[0m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/contrib/capacity-scheduler/*.jar
[36;1mnamenode                     |[0m STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r cd915e1e8d9d0131462a0b7301586c175728a282; compiled by 'kshvachk' on 2017-08-01T00:29Z
[36;1mnamenode                     |[0m STARTUP_MSG:   java = 1.8.0_131
[36;1mnamenode                     |[0m ************************************************************/
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO namenode.NameNode: createNameNode []
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO impl.MetricsSystemImpl: NameNode metrics system started
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO namenode.NameNode: fs.defaultFS is hdfs://namenode:8020
[36;1mnamenode                     |[0m 20/12/04 14:22:52 INFO namenode.NameNode: Clients are to use namenode:8020 to access this namenode/service.
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO http.HttpServer2: Jetty bound to port 50070
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO mortbay.log: jetty-6.1.26
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
[36;1mnamenode                     |[0m 20/12/04 14:22:53 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36;1mnamenode                     |[0m 20/12/04 14:22:53 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: No KeyProvider found.
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: fsLock is fair: true
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Dec 04 14:22:53
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO util.GSet: Computing capacity for map BlocksMap
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO util.GSet: VM type       = 64-bit
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO util.GSet: capacity      = 2^21 = 2097152 entries
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: defaultReplication         = 3
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: maxReplication             = 512
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: minReplication             = 1
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: supergroup          = supergroup
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: isPermissionEnabled = false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: HA Enabled: false
[36;1mnamenode                     |[0m 20/12/04 14:22:53 INFO namenode.FSNamesystem: Append Enabled: true
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: Computing capacity for map INodeMap
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: VM type       = 64-bit
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: capacity      = 2^20 = 1048576 entries
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSDirectory: ACLs enabled? false
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSDirectory: XAttrs enabled? true
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.NameNode: Caching file names occuring more than 10 times
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: Computing capacity for map cachedBlocks
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: VM type       = 64-bit
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: capacity      = 2^18 = 262144 entries
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: Computing capacity for map NameNodeRetryCache
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: VM type       = 64-bit
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO util.GSet: capacity      = 2^15 = 32768 entries
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO common.Storage: Lock on /hadoop/dfs/name/in_use.lock acquired by nodename 226@7b64f85afdef
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FileJournalManager: Recovering unfinalized segments in /hadoop/dfs/name/current
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/Columns$Serializer.deserializeLargeSubset (Lorg/apache/cassandra/io/util/DataInputPlus;Lorg/apache/cassandra/db/Columns;I)Lorg/apache/cassandra/db/Columns;
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/Columns$Serializer.serializeLargeSubset (Ljava/util/Collection;ILorg/apache/cassandra/db/Columns;ILorg/apache/cassandra/io/util/DataOutputPlus;)V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/Columns$Serializer.serializeLargeSubsetSize (Ljava/util/Collection;ILorg/apache/cassandra/db/Columns;I)I
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/commitlog/AbstractCommitLogSegmentManager.advanceAllocatingFrom (Lorg/apache/cassandra/db/commitlog/CommitLogSegment;)V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/transform/BaseIterator.tryGetMoreContents ()Z
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/transform/StoppingTransformation.stop ()V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/db/transform/StoppingTransformation.stopInPartition ()V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/io/util/BufferedDataOutputStreamPlus.doFlush (I)V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/io/util/BufferedDataOutputStreamPlus.writeExcessSlow ()V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/io/util/BufferedDataOutputStreamPlus.writeSlow (JI)V
[32mfeature_store_cassandra      |[0m CompilerOracle: dontinline org/apache/cassandra/io/util/RebufferingInputStream.readPrimitiveSlowly (I)J
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/db/rows/UnfilteredSerializer.serializeRowBody (Lorg/apache/cassandra/db/rows/Row;ILorg/apache/cassandra/db/SerializationHeader;Lorg/apache/cassandra/io/util/DataOutputPlus;)V
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/io/util/Memory.checkBounds (JJ)V
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/io/util/SafeMemory.checkBounds (JJ)V
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/AsymmetricOrdering.selectBoundary (Lorg/apache/cassandra/utils/AsymmetricOrdering/Op;II)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/AsymmetricOrdering.strictnessOfLessThan (Lorg/apache/cassandra/utils/AsymmetricOrdering/Op;)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/BloomFilter.indexes (Lorg/apache/cassandra/utils/IFilter/FilterKey;)[J
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/BloomFilter.setIndexes (JJIJ[J)V
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compare (Ljava/nio/ByteBuffer;[B)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compare ([BLjava/nio/ByteBuffer;)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compareUnsigned (Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/lang/Object;JILjava/lang/Object;JI)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/lang/Object;JILjava/nio/ByteBuffer;)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;)I
[32mfeature_store_cassandra      |[0m CompilerOracle: inline org/apache/cassandra/utils/vint/VIntCoding.encodeVInt (JI)[B
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FileJournalManager: Finalizing edits file /hadoop/dfs/name/current/edits_inprogress_0000000000000000024 -> /hadoop/dfs/name/current/edits_0000000000000000024-0000000000000000026
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/hadoop/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.
[33;1mpyspark-app                  |[0m WARNING: An illegal reflective access operation has occurred
[33;1mpyspark-app                  |[0m WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[33;1mpyspark-app                  |[0m WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[33;1mpyspark-app                  |[0m WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[33;1mpyspark-app                  |[0m WARNING: All illegal access operations will be denied in a future release
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Loaded image for txid 0 from /hadoop/dfs/name/current/fsimage_0000000000000000000
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@698122b2 expecting start txid #1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000011
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000011' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000011 of size 1048576 edits # 11 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4212a0c8 expecting start txid #12
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000012-0000000000000000014
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000012-0000000000000000014' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000012-0000000000000000014 of size 1048576 edits # 3 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1e7aa82b expecting start txid #15
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000015-0000000000000000017
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000015-0000000000000000017' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000015-0000000000000000017 of size 1048576 edits # 3 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b2c0e88 expecting start txid #18
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000018-0000000000000000020
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000018-0000000000000000020' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000018-0000000000000000020 of size 1048576 edits # 3 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5bd82fed expecting start txid #21
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000021-0000000000000000023
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000021-0000000000000000023' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000021-0000000000000000023 of size 1048576 edits # 3 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@c1bd0be expecting start txid #24
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000024-0000000000000000026
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.EditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000024-0000000000000000026' to transaction ID 1
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSImage: Edits file /hadoop/dfs/name/current/edits_0000000000000000024-0000000000000000026 of size 1048576 edits # 3 loaded in 0 seconds
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
[36;1mnamenode                     |[0m 20/12/04 14:22:54 INFO namenode.FSEditLog: Starting log segment at 27
[33;1mpyspark-app                  |[0m Ivy Default Cache set to: /home/jovyan/.ivy2/cache
[33;1mpyspark-app                  |[0m The jars for the packages stored in: /home/jovyan/.ivy2/jars
[33;1mpyspark-app                  |[0m :: loading settings :: url = jar:file:/usr/local/spark-3.0.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.NameCache: initialized with 0 entries 0 lookups
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.FSNamesystem: Finished loading FSImage in 1105 msecs
[33;1mpyspark-app                  |[0m org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[33;1mpyspark-app                  |[0m com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[33;1mpyspark-app                  |[0m :: resolving dependencies :: org.apache.spark#spark-submit-parent-aa6dbc91-46e4-4509-910e-ecc4045fc232;1.0
[33;1mpyspark-app                  |[0m 	confs: [default]
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:8020
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO ipc.Server: Starting Socket Reader #1 for port 8020
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.LeaseManager: Number of blocks under construction: 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.FSNamesystem: initializing replication queues
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO hdfs.StateChange: STATE* Leaving safe mode after 1 secs
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.BlockManager: Total number of blocks            = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.BlockManager: Number of blocks being written    = 0
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 64 msec
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO ipc.Server: IPC Server Responder: starting
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO ipc.Server: IPC Server listener on 8020: starting
[33;1mpyspark-app                  |[0m 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:55,884 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/cassandra/cassandra.yaml
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.NameNode: NameNode RPC up at: namenode/172.21.0.4:8020
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO namenode.FSNamesystem: Starting services required for active state
[36;1mnamenode                     |[0m 20/12/04 14:22:55 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
[33;1mpyspark-app                  |[0m 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central
[33;1mpyspark-app                  |[0m 	found org.apache.kafka#kafka-clients;2.4.1 in central
[33;1mpyspark-app                  |[0m 	found com.github.luben#zstd-jni;1.4.4-3 in central
[33;1mpyspark-app                  |[0m 	found org.lz4#lz4-java;1.7.1 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,530 Config.java:534 - Node configuration:[allocate_tokens_for_keyspace=null; authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=false; back_pressure_strategy=org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}; batch_size_fail_threshold_in_kb=50; batch_size_warn_threshold_in_kb=5; batchlog_replay_throttle_in_kb=1024; broadcast_address=172.21.0.5; broadcast_rpc_address=172.21.0.5; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=1000; cdc_enabled=false; cdc_free_space_check_interval_ms=250; cdc_raw_directory=null; cdc_total_space_in_mb=0; check_for_duplicate_rows_during_compaction=true; check_for_duplicate_rows_during_reads=true; client_encryption_options=<REDACTED>; cluster_name=Test Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=null; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=NaN; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=16; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@3d299e3; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_materialized_views=true; enable_sasi_indexes=true; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=SimpleSnitch; file_cache_round_up=null; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=null; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=0; internode_send_buff_size_in_bytes=0; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=172.21.0.5; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; min_free_space_per_drive_in_mb=50; native_transport_flush_in_batches_legacy=true; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_concurrent_requests_in_bytes=-1; native_transport_max_concurrent_requests_in_bytes_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_negotiable_protocol_version=-2147483648; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=256; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; repair_session_max_tree_depth=18; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=10000; role_manager=CassandraRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=0.0.0.0; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=null; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=172.21.0.5}; server_encryption_options=<REDACTED>; slow_query_log_timeout_in_ms=500; snapshot_before_compaction=false; snapshot_on_duplicate_row_detection=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=false; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_keep_alive_period_in_secs=300; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@55a561cf; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=2000]
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,531 DatabaseDescriptor.java:381 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,532 DatabaseDescriptor.java:439 - Global memtable on-heap threshold is enabled at 1224MB
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,532 DatabaseDescriptor.java:443 - Global memtable off-heap threshold is enabled at 1224MB
[33;1mpyspark-app                  |[0m 	found org.xerial.snappy#snappy-java;1.1.7.5 in central
[33mdatanode                     |[0m [2/100] namenode:50070 is available.
[33;1mpyspark-app                  |[0m 	found org.slf4j#slf4j-api;1.7.30 in central
[33;1mpyspark-app                  |[0m 	found org.spark-project.spark#unused;1.0.0 in central
[33;1mpyspark-app                  |[0m 	found org.apache.commons#commons-pool2;2.6.2 in central
[35;1mzookeeper                    |[0m ===> Running preflight checks ... 
[33;1mpyspark-app                  |[0m 	found com.datastax.spark#spark-cassandra-connector_2.12;3.0.0-beta in central
[35;1mzookeeper                    |[0m ===> Check if /var/lib/zookeeper/data is writable ...
[33;1mpyspark-app                  |[0m 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.0.0-beta in central
[33;1mpyspark-app                  |[0m 	found com.datastax.oss#java-driver-core-shaded;4.7.2 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,815 RateBasedBackPressure.java:123 - Initialized back-pressure with high ratio: 0.9, factor: 5, flow: FAST, window size: 2000.
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:56,816 DatabaseDescriptor.java:773 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure{high_ratio=0.9, factor=5, flow=FAST}.
[33;1mpyspark-app                  |[0m 	found com.datastax.oss#native-protocol;1.4.10 in central
[33;1mpyspark-app                  |[0m 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[33;1mpyspark-app                  |[0m 	found com.typesafe#config;1.3.4 in central
[33;1mpyspark-app                  |[0m 	found io.dropwizard.metrics#metrics-core;4.0.5 in central
[33;1mpyspark-app                  |[0m 	found org.hdrhistogram#HdrHistogram;2.1.11 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,140 JMXServerUtils.java:252 - Configured JMX server at: service:jmx:rmi://127.0.0.1/jndi/rmi://127.0.0.1:7199/jmxrmi
[33;1mpyspark-app                  |[0m 	found org.javatuples#javatuples;1.2 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,183 CassandraDaemon.java:490 - Hostname: 9ef00a40f123
[35mhive-metastore               |[0m [2/100] namenode:50070 is available.
[35mhive-metastore               |[0m [1/100] check for datanode:50075...
[35mhive-metastore               |[0m [1/100] datanode:50075 is not available yet
[35mhive-metastore               |[0m [1/100] try in 5s once again ...
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,203 CassandraDaemon.java:497 - JVM vendor/version: OpenJDK 64-Bit Server VM/1.8.0_262
[33;1mpyspark-app                  |[0m 	found org.reactivestreams#reactive-streams;1.0.2 in central
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,228 CassandraDaemon.java:498 - Heap size: 4.781GiB/4.781GiB
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,234 CassandraDaemon.java:503 - Code Cache Non-heap memory: init = 2555904(2496K) used = 4378432(4275K) committed = 4390912(4288K) max = 251658240(245760K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,236 CassandraDaemon.java:503 - Metaspace Non-heap memory: init = 0(0K) used = 19021040(18575K) committed = 19529728(19072K) max = -1(-1K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,256 CassandraDaemon.java:503 - Compressed Class Space Non-heap memory: init = 0(0K) used = 2297432(2243K) committed = 2490368(2432K) max = 1073741824(1048576K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,259 CassandraDaemon.java:503 - Par Eden Space Heap memory: init = 671088640(655360K) used = 147659216(144198K) committed = 671088640(655360K) max = 671088640(655360K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,259 CassandraDaemon.java:503 - Par Survivor Space Heap memory: init = 83886080(81920K) used = 0(0K) committed = 83886080(81920K) max = 83886080(81920K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,260 CassandraDaemon.java:503 - CMS Old Gen Heap memory: init = 4378853376(4276224K) used = 0(0K) committed = 4378853376(4276224K) max = 4378853376(4276224K)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,270 CassandraDaemon.java:505 - Classpath: /etc/cassandra:/opt/cassandra/build/classes/main:/opt/cassandra/build/classes/thrift:/opt/cassandra/lib/HdrHistogram-2.1.9.jar:/opt/cassandra/lib/ST4-4.0.8.jar:/opt/cassandra/lib/airline-0.6.jar:/opt/cassandra/lib/antlr-runtime-3.5.2.jar:/opt/cassandra/lib/apache-cassandra-3.11.7.jar:/opt/cassandra/lib/apache-cassandra-thrift-3.11.7.jar:/opt/cassandra/lib/asm-5.0.4.jar:/opt/cassandra/lib/caffeine-2.2.6.jar:/opt/cassandra/lib/cassandra-driver-core-3.0.1-shaded.jar:/opt/cassandra/lib/commons-cli-1.1.jar:/opt/cassandra/lib/commons-codec-1.9.jar:/opt/cassandra/lib/commons-lang3-3.1.jar:/opt/cassandra/lib/commons-math3-3.2.jar:/opt/cassandra/lib/compress-lzf-0.8.4.jar:/opt/cassandra/lib/concurrent-trees-2.4.0.jar:/opt/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/opt/cassandra/lib/disruptor-3.0.1.jar:/opt/cassandra/lib/ecj-4.4.2.jar:/opt/cassandra/lib/guava-18.0.jar:/opt/cassandra/lib/high-scale-lib-1.0.6.jar:/opt/cassandra/lib/hppc-0.5.4.jar:/opt/cassandra/lib/jackson-annotations-2.9.10.jar:/opt/cassandra/lib/jackson-core-2.9.10.jar:/opt/cassandra/lib/jackson-databind-2.9.10.4.jar:/opt/cassandra/lib/jamm-0.3.0.jar:/opt/cassandra/lib/javax.inject.jar:/opt/cassandra/lib/jbcrypt-0.3m.jar:/opt/cassandra/lib/jcl-over-slf4j-1.7.7.jar:/opt/cassandra/lib/jctools-core-1.2.1.jar:/opt/cassandra/lib/jflex-1.6.0.jar:/opt/cassandra/lib/jna-4.2.2.jar:/opt/cassandra/lib/joda-time-2.4.jar:/opt/cassandra/lib/json-simple-1.1.jar:/opt/cassandra/lib/jstackjunit-0.0.1.jar:/opt/cassandra/lib/libthrift-0.9.2.jar:/opt/cassandra/lib/log4j-over-slf4j-1.7.7.jar:/opt/cassandra/lib/logback-classic-1.1.3.jar:/opt/cassandra/lib/logback-core-1.1.3.jar:/opt/cassandra/lib/lz4-1.3.0.jar:/opt/cassandra/lib/metrics-core-3.1.5.jar:/opt/cassandra/lib/metrics-jvm-3.1.5.jar:/opt/cassandra/lib/metrics-logback-3.1.5.jar:/opt/cassandra/lib/netty-all-4.0.44.Final.jar:/opt/cassandra/lib/ohc-core-0.4.4.jar:/opt/cassandra/lib/ohc-core-j8-0.4.4.jar:/opt/cassandra/lib/reporter-config-base-3.0.3.jar:/opt/cassandra/lib/reporter-config3-3.0.3.jar:/opt/cassandra/lib/sigar-1.6.4.jar:/opt/cassandra/lib/slf4j-api-1.7.7.jar:/opt/cassandra/lib/snakeyaml-1.11.jar:/opt/cassandra/lib/snappy-java-1.1.1.7.jar:/opt/cassandra/lib/snowball-stemmer-1.3.0.581.1.jar:/opt/cassandra/lib/stream-2.5.2.jar:/opt/cassandra/lib/thrift-server-0.3.7.jar:/opt/cassandra/lib/jsr223/*/*.jar::/opt/cassandra/lib/jamm-0.3.0.jar
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,271 CassandraDaemon.java:507 - JVM Arguments: [-Xloggc:/opt/cassandra/logs/gc.log, -ea, -XX:+UseThreadPriorities, -XX:ThreadPriorityPolicy=42, -XX:+HeapDumpOnOutOfMemoryError, -Xss256k, -XX:StringTableSize=1000003, -XX:+AlwaysPreTouch, -XX:-UseBiasedLocking, -XX:+UseTLAB, -XX:+ResizeTLAB, -XX:+UseNUMA, -XX:+PerfDisableSharedMem, -Djava.net.preferIPv4Stack=true, -XX:+UseParNewGC, -XX:+UseConcMarkSweepGC, -XX:+CMSParallelRemarkEnabled, -XX:SurvivorRatio=8, -XX:MaxTenuringThreshold=1, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:CMSWaitDuration=10000, -XX:+CMSParallelInitialMarkEnabled, -XX:+CMSEdenChunksRecordAlways, -XX:+CMSClassUnloadingEnabled, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintHeapAtGC, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -XX:+PrintPromotionFailure, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=10, -XX:GCLogFileSize=10M, -Xms4975M, -Xmx4975M, -Xmn800M, -XX:+UseCondCardMark, -XX:CompileCommandFile=/etc/cassandra/hotspot_compiler, -javaagent:/opt/cassandra/lib/jamm-0.3.0.jar, -Dcassandra.jmx.local.port=7199, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password, -Djava.library.path=/opt/cassandra/lib/sigar-bin, -Dcassandra.libjemalloc=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1, -XX:OnOutOfMemoryError=kill -9 %p, -Dlogback.configurationFile=logback.xml, -Dcassandra.logdir=/opt/cassandra/logs, -Dcassandra.storagedir=/opt/cassandra/data, -Dcassandra-foreground=yes]
[34mhive-server                  |[0m [2/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [2/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [2/100] try in 5s once again ...
[33;1mpyspark-app                  |[0m 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[33;1mpyspark-app                  |[0m 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[33;1mpyspark-app                  |[0m 	found com.google.code.findbugs#jsr305;3.0.2 in central
[32mfeature_store_cassandra      |[0m WARN  [main] 2020-12-04 14:22:57,467 NativeLibrary.java:189 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,471 StartupChecks.java:140 - jemalloc seems to be preloaded from /usr/lib/x86_64-linux-gnu/libjemalloc.so.1
[32mfeature_store_cassandra      |[0m WARN  [main] 2020-12-04 14:22:57,475 StartupChecks.java:169 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:57,497 SigarLibrary.java:44 - Initializing SIGAR library
[33;1mpyspark-app                  |[0m 	found com.datastax.oss#java-driver-mapper-runtime;4.7.2 in central
[32mfeature_store_cassandra      |[0m WARN  [main] 2020-12-04 14:22:57,545 SigarLibrary.java:174 - Cassandra server running in degraded mode. Is swap disabled? : false,  Address space adequate? : true,  nofile limit adequate? : true, nproc limit adequate? : true 
[32mfeature_store_cassandra      |[0m WARN  [main] 2020-12-04 14:22:57,561 StartupChecks.java:311 - Maximum number of memory map areas per process (vm.max_map_count) 65530 is too low, recommended value: 1048575, you can change it with sysctl.
[33;1mpyspark-app                  |[0m 	found com.datastax.oss#java-driver-query-builder;4.7.2 in central
[33;1mpyspark-app                  |[0m 	found org.apache.commons#commons-lang3;3.5 in central
[33;1mpyspark-app                  |[0m 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[33;1mpyspark-app                  |[0m 	found org.scala-lang#scala-reflect;2.12.11 in central
[33;1mpyspark-app                  |[0m :: resolution report :: resolve 2463ms :: artifacts dl 34ms
[33;1mpyspark-app                  |[0m 	:: modules in use:
[33;1mpyspark-app                  |[0m 	com.datastax.oss#java-driver-core-shaded;4.7.2 from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.oss#java-driver-mapper-runtime;4.7.2 from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.oss#java-driver-query-builder;4.7.2 from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.oss#native-protocol;1.4.10 from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.0.0-beta from central in [default]
[33;1mpyspark-app                  |[0m 	com.datastax.spark#spark-cassandra-connector_2.12;3.0.0-beta from central in [default]
[33;1mpyspark-app                  |[0m 	com.github.luben#zstd-jni;1.4.4-3 from central in [default]
[33;1mpyspark-app                  |[0m 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[33;1mpyspark-app                  |[0m 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[33;1mpyspark-app                  |[0m 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[33;1mpyspark-app                  |[0m 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[33;1mpyspark-app                  |[0m 	com.typesafe#config;1.3.4 from central in [default]
[33;1mpyspark-app                  |[0m 	io.dropwizard.metrics#metrics-core;4.0.5 from central in [default]
[33;1mpyspark-app                  |[0m 	org.apache.commons#commons-lang3;3.5 from central in [default]
[33;1mpyspark-app                  |[0m 	org.apache.commons#commons-pool2;2.6.2 from central in [default]
[33;1mpyspark-app                  |[0m 	org.apache.kafka#kafka-clients;2.4.1 from central in [default]
[33;1mpyspark-app                  |[0m 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]
[33;1mpyspark-app                  |[0m 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]
[33;1mpyspark-app                  |[0m 	org.hdrhistogram#HdrHistogram;2.1.11 from central in [default]
[33;1mpyspark-app                  |[0m 	org.javatuples#javatuples;1.2 from central in [default]
[33;1mpyspark-app                  |[0m 	org.lz4#lz4-java;1.7.1 from central in [default]
[33;1mpyspark-app                  |[0m 	org.reactivestreams#reactive-streams;1.0.2 from central in [default]
[33;1mpyspark-app                  |[0m 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[33;1mpyspark-app                  |[0m 	org.slf4j#slf4j-api;1.7.30 from central in [default]
[33;1mpyspark-app                  |[0m 	org.spark-project.spark#unused;1.0.0 from central in [default]
[33;1mpyspark-app                  |[0m 	org.xerial.snappy#snappy-java;1.1.7.5 from central in [default]
[33;1mpyspark-app                  |[0m 	:: evicted modules:
[33;1mpyspark-app                  |[0m 	org.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.30] in [default]
[33;1mpyspark-app                  |[0m 	---------------------------------------------------------------------
[33;1mpyspark-app                  |[0m 	|                  |            modules            ||   artifacts   |
[33;1mpyspark-app                  |[0m 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[33;1mpyspark-app                  |[0m 	---------------------------------------------------------------------
[33;1mpyspark-app                  |[0m 	|      default     |   28  |   0   |   0   |   1   ||   27  |   0   |
[33;1mpyspark-app                  |[0m 	---------------------------------------------------------------------
[33;1mpyspark-app                  |[0m :: retrieving :: org.apache.spark#spark-submit-parent-aa6dbc91-46e4-4509-910e-ecc4045fc232
[33;1mpyspark-app                  |[0m 	confs: [default]
[33;1mpyspark-app                  |[0m 	0 artifacts copied, 27 already retrieved (0kB/66ms)
[35;1mzookeeper                    |[0m ===> Check if /var/lib/zookeeper/log is writable ...
[33mdatanode                     |[0m 20/12/04 14:22:58 INFO datanode.DataNode: STARTUP_MSG: 
[33mdatanode                     |[0m /************************************************************
[33mdatanode                     |[0m STARTUP_MSG: Starting DataNode
[33mdatanode                     |[0m STARTUP_MSG:   host = ec71a4c4aa73/172.21.0.3
[33mdatanode                     |[0m STARTUP_MSG:   args = []
[33mdatanode                     |[0m STARTUP_MSG:   version = 2.7.4
[33mdatanode                     |[0m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/contrib/capacity-scheduler/*.jar
[33mdatanode                     |[0m STARTUP_MSG:   build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r cd915e1e8d9d0131462a0b7301586c175728a282; compiled by 'kshvachk' on 2017-08-01T00:29Z
[33mdatanode                     |[0m STARTUP_MSG:   java = 1.8.0_131
[33mdatanode                     |[0m ************************************************************/
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:58,050 QueryProcessor.java:116 - Initialized prepared statement caches with 19 MB (native) and 19 MB (Thrift)
[33mdatanode                     |[0m 20/12/04 14:22:58 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
[33;1mpyspark-app                  |[0m 20/12/04 14:22:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[35;1mzookeeper                    |[0m ===> Launching ... 
[35;1mzookeeper                    |[0m ===> Launching zookeeper ... 
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:22:59,205 ColumnFamilyStore.java:427 - Initializing system.IndexInfo
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO impl.MetricsSystemImpl: DataNode metrics system started
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.DataNode: Configured hostname is ec71a4c4aa73
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
[33mdatanode                     |[0m 20/12/04 14:22:59 INFO datanode.DataNode: Number threads for balancing is 5
[36mbroker                       |[0m ===> Running preflight checks ... 
[36mbroker                       |[0m ===> Check if /var/lib/kafka/data is writable ...
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33;1mpyspark-app                  |[0m Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[33;1mpyspark-app                  |[0m Setting default log level to "WARN".
[33;1mpyspark-app                  |[0m To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO http.HttpServer2: Jetty bound to port 44819
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO mortbay.log: jetty-6.1.26
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44819
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:00,785 ColumnFamilyStore.java:427 - Initializing system.batches
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:00,812 ColumnFamilyStore.java:427 - Initializing system.paxos
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,816] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,870] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,871] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO datanode.DataNode: dnUserName = root
[33mdatanode                     |[0m 20/12/04 14:23:00 INFO datanode.DataNode: supergroup = supergroup
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,898] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,898] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,898] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,898] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:00,913 ColumnFamilyStore.java:427 - Initializing system.local
[35;1mzookeeper                    |[0m [2020-12-04 14:23:00,944] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,038] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,039] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,039] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,039] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,087] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO ipc.Server: Starting Socket Reader #1 for port 50020
[32mfeature_store_cassandra      |[0m INFO  [SSTableBatchOpen:1] 2020-12-04 14:23:01,147 BufferPool.java:234 - Global buffer pool is enabled, when pool is exhausted (max is 512.000MiB) it will allocate on heap
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,187] INFO Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,187] INFO Server environment:host.name=efcb720c5c96 (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,189] INFO Server environment:java.version=1.8.0_212 (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,190] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,193] INFO Server environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,194] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/connect-json-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-common-2.28.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.48.Final.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.2.jar:/usr/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/usr/bin/../share/java/kafka/jersey-server-2.28.jar:/usr/bin/../share/java/kafka/connect-api-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/avro-1.9.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.48.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-javadoc.jar:/usr/bin/../share/java/kafka/commons-codec-1.11.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.48.Final.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/connect-runtime-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.2.jar:/usr/bin/../share/java/kafka/support-metrics-common-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.2.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-sources.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/scala-library-2.12.10.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-test.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/connect-file-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.2.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/connect-transforms-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.2.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.48.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/usr/bin/../share/java/kafka/jersey-client-2.28.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/support-metrics-client-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/kafka-clients-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/kafka-streams-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.12-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.2.jar:/usr/bin/../share/java/kafka/httpclient-4.5.11.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/commons-compress-1.19.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.48.Final.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-scaladoc.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.2.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.12-2.10.2.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.2.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/netty-common-4.1.48.Final.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-tools-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.48.Final.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/connect-mirror-client-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-test-sources.jar:/usr/bin/../share/java/kafka/httpmime-4.5.11.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.28.jar:/usr/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/usr/bin/../share/java/kafka/javassist-3.22.0-CR2.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/connect-mirror-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.12/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/* (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,197] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,198] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,199] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,200] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,201] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,201] INFO Server environment:os.version=5.4.0-56-generic (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,201] INFO Server environment:user.name=root (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,201] INFO Server environment:user.home=/root (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,202] INFO Server environment:user.dir=/ (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,202] INFO Server environment:os.memory.free=499MB (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,203] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,203] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,218] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,220] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,230] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020
[35;1mzookeeper                    |[0m [2020-12-04 14:23:01,409] INFO Logging initialized @2031ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO datanode.DataNode: Refresh request received for nameservices: null
[36mbroker                       |[0m ===> Check if Zookeeper is healthy ...
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to namenode/172.21.0.4:8020 starting to offer service
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO ipc.Server: IPC Server Responder: starting
[33mdatanode                     |[0m 20/12/04 14:23:01 INFO ipc.Server: IPC Server listener on 50020: starting
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,005] WARN o.e.j.s.ServletContextHandler@74650e52{/,null,UNAVAILABLE} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,005] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,050 CacheService.java:100 - Initializing key cache with capacity of 100 MBs.
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,053] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 1.8.0_212-b04 (org.eclipse.jetty.server.Server)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,078 CacheService.java:122 - Initializing row cache with capacity of 0 MBs
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,083 CacheService.java:151 - Initializing counter cache with capacity of 50 MBs
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,087 CacheService.java:162 - Scheduling counter cache save to every 7200 seconds (going to save all keys).
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,125 ColumnFamilyStore.java:427 - Initializing system.peers
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,153 ColumnFamilyStore.java:427 - Initializing system.peer_events
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,205 ColumnFamilyStore.java:427 - Initializing system.range_xfers
[35mhive-metastore               |[0m [2/100] datanode:50075 is available.
[32;1mhive-metastore-postgresql_1  |[0m LOG:  incomplete startup packet
[35mhive-metastore               |[0m [1/100] hive-metastore-postgresql:5432 is available.
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,276 ColumnFamilyStore.java:427 - Initializing system.compaction_history
[34mhive-server                  |[0m [3/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [3/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [3/100] try in 5s once again ...
[35mhive-metastore               |[0m 2020-12-04 14:23:02: Starting Hive Metastore Server
[35mhive-metastore               |[0m /opt/hive/bin/ext/metastore.sh: line 29: export: ` -Dproc_metastore  -Dlog4j.configurationFile=hive-log4j2.properties  -Djava.util.logging.config.file=/opt/hive/conf/parquet-logging.properties  ': not a valid identifier
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,445 ColumnFamilyStore.java:427 - Initializing system.sstable_activity
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,538] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,542] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,550] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,601 ColumnFamilyStore.java:427 - Initializing system.size_estimates
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,626] INFO Started o.e.j.s.ServletContextHandler@74650e52{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,714] INFO Started ServerConnector@31dc339b{HTTP/1.1,[http/1.1]}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,719] INFO Started @3341ms (org.eclipse.jetty.server.Server)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,723] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,755] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,763] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 16 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,767] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,782 ColumnFamilyStore.java:427 - Initializing system.available_ranges
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,808 ColumnFamilyStore.java:427 - Initializing system.transferred_ranges
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,844] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,852] INFO Reading snapshot /var/lib/zookeeper/data/version-2/snapshot.76 (org.apache.zookeeper.server.persistence.FileSnap)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,878 ColumnFamilyStore.java:427 - Initializing system.views_builds_in_progress
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,912 ColumnFamilyStore.java:427 - Initializing system.built_views
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:02,936 ColumnFamilyStore.java:427 - Initializing system.hints
[35;1mzookeeper                    |[0m [2020-12-04 14:23:02,968] INFO Snapshotting: 0x8b to /var/lib/zookeeper/data/version-2/snapshot.8b (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[35;1mzookeeper                    |[0m [2020-12-04 14:23:03,090] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,163 ColumnFamilyStore.java:427 - Initializing system.batchlog
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,194 ColumnFamilyStore.java:427 - Initializing system.prepared_statements
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO common.Storage: Lock on /hadoop/dfs/data/in_use.lock acquired by nodename 228@ec71a4c4aa73
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,216 ColumnFamilyStore.java:427 - Initializing system.schema_keyspaces
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,239 ColumnFamilyStore.java:427 - Initializing system.schema_columnfamilies
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,264 ColumnFamilyStore.java:427 - Initializing system.schema_columns
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,283 ColumnFamilyStore.java:427 - Initializing system.schema_triggers
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,302 ColumnFamilyStore.java:427 - Initializing system.schema_usertypes
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,315 ColumnFamilyStore.java:427 - Initializing system.schema_functions
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,329 ColumnFamilyStore.java:427 - Initializing system.schema_aggregates
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,334 ViewManager.java:137 - Not submitting build tasks for views in keyspace system as storage service is not initialized
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO common.Storage: Analyzing storage directories for bpid BP-949235150-172.21.0.8-1607088751745
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO common.Storage: Locking is disabled for /hadoop/dfs/data/current/BP-949235150-172.21.0.8-1607088751745
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO datanode.DataNode: Setting up storage: nsid=1073995652;bpid=BP-949235150-172.21.0.8-1607088751745;lv=-56;nsInfo=lv=-63;cid=CID-f9cf6db2-718c-466d-948d-83c94bc7c7f6;nsid=1073995652;c=0;bpid=BP-949235150-172.21.0.8-1607088751745;dnuuid=55eb0f22-95aa-40e8-a69c-ff5ce798fa33
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Added new volume: DS-f0d03f00-f495-446c-9f1a-503931331604
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Added volume - /hadoop/dfs/data/current, StorageType: DISK
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Adding block pool BP-949235150-172.21.0.8-1607088751745
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Scanning block pool BP-949235150-172.21.0.8-1607088751745 on volume /hadoop/dfs/data/current...
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-949235150-172.21.0.8-1607088751745 on /hadoop/dfs/data/current: 78ms
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-949235150-172.21.0.8-1607088751745: 97ms
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-949235150-172.21.0.8-1607088751745 on volume /hadoop/dfs/data/current...
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-949235150-172.21.0.8-1607088751745 on volume /hadoop/dfs/data/current: 0ms
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO impl.FsDatasetImpl: Total time to add all replicas to map: 16ms
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=2fe9a2f7062b
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.8.0_212
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=5.4.0-56-generic
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=294MB
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=4423MB
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=299MB
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@cc34f4d
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:03,910 ApproximateTime.java:44 - Scheduling approximate time-check task with a precision of 10 milliseconds
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-f0d03f00-f495-446c-9f1a-503931331604): no suitable block pools found to scan.  Waiting 1811378130 ms.
[33mdatanode                     |[0m 20/12/04 14:23:03 ERROR datanode.DirectoryScanner: dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value below 1 ms/sec. Assuming default value of 1000
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1607092947976ms with interval of 21600000ms
[33mdatanode                     |[0m 20/12/04 14:23:03 INFO datanode.DataNode: Block pool BP-949235150-172.21.0.8-1607088751745 (Datanode Uuid null) service to namenode/172.21.0.4:8020 beginning handshake with NN
[36mbroker                       |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.21.0.6:2181. Will not attempt to authenticate using SASL (unknown error)
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.3:50010, datanodeUuid=55eb0f22-95aa-40e8-a69c-ff5ce798fa33, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f9cf6db2-718c-466d-948d-83c94bc7c7f6;nsid=1073995652;c=0) storage 55eb0f22-95aa-40e8-a69c-ff5ce798fa33
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO net.NetworkTopology: Adding a new node: /default-rack/172.21.0.3:50010
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: Block pool Block pool BP-949235150-172.21.0.8-1607088751745 (Datanode Uuid null) service to namenode/172.21.0.4:8020 successfully registered with NN
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: For namenode namenode/172.21.0.4:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,235 ColumnFamilyStore.java:427 - Initializing system_schema.keyspaces
[36mbroker                       |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.21.0.10:35052, server: zookeeper/172.21.0.6:2181
[35;1mzookeeper                    |[0m [2020-12-04 14:23:04,280] INFO Creating new log file: log.8c (org.apache.zookeeper.server.persistence.FileTxnLog)
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f0d03f00-f495-446c-9f1a-503931331604 for DN 172.21.0.3:50010
[36mbroker                       |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.21.0.6:2181, sessionid = 0x100021a15d10000, negotiated timeout = 40000
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,310 ColumnFamilyStore.java:427 - Initializing system_schema.tables
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: Namenode Block pool BP-949235150-172.21.0.8-1607088751745 (Datanode Uuid 55eb0f22-95aa-40e8-a69c-ff5ce798fa33) service to namenode/172.21.0.4:8020 trying to claim ACTIVE state with txid=27
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-949235150-172.21.0.8-1607088751745 (Datanode Uuid 55eb0f22-95aa-40e8-a69c-ff5ce798fa33) service to namenode/172.21.0.4:8020
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,374 ColumnFamilyStore.java:427 - Initializing system_schema.columns
[36;1mnamenode                     |[0m 20/12/04 14:23:04 INFO BlockStateChange: BLOCK* processReport 0x2012d9cbb258: from storage DS-f0d03f00-f495-446c-9f1a-503931331604 node DatanodeRegistration(172.21.0.3:50010, datanodeUuid=55eb0f22-95aa-40e8-a69c-ff5ce798fa33, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f9cf6db2-718c-466d-948d-83c94bc7c7f6;nsid=1073995652;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,450 ColumnFamilyStore.java:427 - Initializing system_schema.triggers
[36mbroker                       |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100021a15d10000 closed
[36mbroker                       |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100021a15d10000
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: Successfully sent block report 0x2012d9cbb258,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 6 msec to generate and 146 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
[33mdatanode                     |[0m 20/12/04 14:23:04 INFO datanode.DataNode: Got finalize command for block pool BP-949235150-172.21.0.8-1607088751745
[36mbroker                       |[0m ===> Launching ... 
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,531 ColumnFamilyStore.java:427 - Initializing system_schema.dropped_columns
[36mbroker                       |[0m ===> Launching kafka ... 
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,626 ColumnFamilyStore.java:427 - Initializing system_schema.views
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,792 ColumnFamilyStore.java:427 - Initializing system_schema.types
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,844 ColumnFamilyStore.java:427 - Initializing system_schema.functions
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:04,939 ColumnFamilyStore.java:427 - Initializing system_schema.aggregates
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:05,025 ColumnFamilyStore.java:427 - Initializing system_schema.indexes
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:05,076 ViewManager.java:137 - Not submitting build tasks for views in keyspace system_schema as storage service is not initialized
[36mbroker                       |[0m [2020-12-04 14:23:06,524] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:06,543 StorageService.java:639 - Populating token metadata from system tables
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:06,620 StorageService.java:646 - Token metadata: Normal Tokens:
[32mfeature_store_cassandra      |[0m /172.21.0.5:[-9222462279526940729, -9217724739638825635, -9151549961426244900, -9136597833165407377, -9115701091624094086, -8925504728471672670, -8857146325389100826, -8836987850072989671, -8805667914205439360, -8801207003684924489, -8730271329515652010, -8711525909978044341, -8663406423891536409, -8572236876347536000, -8570709853748325001, -8569317954359339232, -8558022788179604962, -8555300652773014355, -8525195348617948484, -8517474275393259658, -8381979369603550132, -8323180521874766055, -8140931423667542528, -8041430497125390797, -7900054234908854530, -7852502768627427179, -7843311331735263809, -7735669254629999051, -7611163841763033590, -7573801158441557875, -7520144948494757138, -7497178269007883140, -7449202655839127769, -7431871051889857143, -7334068495991224547, -7221299466298370676, -7134358937110891445, -7067139616167739020, -6907286070419363831, -6749478975951498955, -6699317236253111511, -6610203117406499617, -6558873882574301314, -6512963201491721095, -6489582167941360675, -6431105386388673139, -6309139966879979605, -6286022595309619674, -6248913881862756859, -6227597895151671991, -6193006078463456758, -5937896629784122110, -5913675184845324495, -5768892355720758654, -5733422764469246883, -5694692026270781080, -5662973566293327903, -5577560613354674553, -5548005055494555263, -5531458897555369546, -5452844751628794944, -5444563961520073368, -5356131254839581908, -5301522322223597370, -5300648707360593976, -5263317125206602369, -5101541359620277030, -4964924868531996037, -4904296620831236635, -4831182040615610739, -4558031089111575335, -4540309932640295789, -4537094550598482077, -4259613591415764324, -4220591557700518966, -4063344387153616626, -4035228948985284538, -3907110743192563053, -3896270440661159709, -3819660455371896284, -3810717760295389677, -3706162385958874607, -3641429811498231099, -3562941904943635297, -3501887220289881588, -3490095103987132272, -3433220685152622432, -3381851930603487669, -3263692190134406809, -3225692547268242018, -3165447613019463353, -2958026493938050239, -2918661169886939133, -2899954717261500426, -2849221656213685949, -2846544481065526154, -2703018455337632603, -2699252867322411682, -2646850752393599998, -2588443582297640847, -2529647815633595278, -2495723744160678763, -2358578714082005207, -2323317641372967838, -2113453126667313805, -2089170347268745925, -2058971224502709416, -2056764132194934214, -2009509707125343309, -1904691198252941024, -1803826244366676965, -1783144622509201634, -1655502394787503524, -1612778609706922906, -1557929684011240014, -1414461316693241715, -1403924273843224846, -1090741041435603633, -1031536213055703161, -797576509160914914, -737460129771881139, -715684530501463534, -704983557997436964, -409641939137021197, -337146694316026790, -271257989842786412, -268972032529423155, -257078192892743442, -207564375975841295, -125708361991687768, -77730594681051849, -32601591101317197, 64256610372887892, 121271338954989756, 202919063969201200, 220346142021851285, 251798523943968962, 294116154372933812, 346759538210938881, 365309986693834111, 452696378713007821, 482479979224523474, 554274804072309445, 608766224905863164, 623441826881715785, 665478613716682803, 669226207046755963, 834889313670944771, 1078967558619006540, 1091873580410695655, 1196108293584585512, 1208889646038188607, 1314306528234485519, 1365930770602746197, 1397143853617564888, 1427806758529221982, 1432959612142320372, 1436895272146396613, 1437302186345542681, 1552667446179808613, 1568466223635171804, 1634934012247699851, 1768749434927680281, 1798944430388274733, 1883585726015123908, 1946429773246929562, 1956961488036787702, 2100267329926644454, 2135897883085063014, 2154251332452768347, 2158970341380225745, 2281329506025306846, 2286320340144235835, 2389327987160494988, 2513774485213321164, 2532890183660193514, 2614257510568677222, 2675131356865280632, 2742009889787740058, 2866631975047529270, 2920224300850566680, 3131328396153992652, 3179682579365636815, 3272921229688016692, 3286200332773786314, 3381911677469042705, 3385996052368622493, 3555826281517098791, 3565116237765911230, 3581687187087825683, 3601763283198459940, 3606533710952620163, 3641868505336578708, 3770338870039414139, 3774090906923649163, 3816534254031220279, 3884191877402592795, 3912346151261754763, 3961030711362906997, 3970998512943429386, 4066258062766552273, 4377038866603429513, 4485520094578593345, 4536311093243180967, 4563358932391685744, 4620423090546473801, 4703855626191875945, 4924609019138089882, 4931669496217960705, 4933162400387698133, 5066177946978027916, 5181614186184117871, 5271031424053411441, 5504839486204956921, 5534046281585578369, 5545215022789318862, 5561881892879566269, 5619585619210582096, 5742453635103018975, 5784091183682637065, 6047315214792652256, 6067414848421684736, 6152549059498546168, 6226947915427227435, 6312680169786497716, 6339746999492721257, 6544433811999462475, 6558451757357966186, 6604769750017214952, 6622098249028689607, 6708717723036651221, 6815865468005635224, 6900737124913345813, 6928676410592872879, 7136205081486591229, 7147976497754259490, 7194045954990991964, 7316653396289246944, 7322640315461134561, 7370051386776012029, 7394212976038461610, 7399782971886267427, 7613750723391114203, 7685610640593201240, 8027858181135039677, 8049387002671091191, 8088100529651816314, 8129574354202449035, 8168965524297561585, 8235916769092824128, 8254124585999613166, 8340217050461616189, 8403356175272744790, 8545027220934141032, 9113990167241861408, 9130233083048470412]
[32mfeature_store_cassandra      |[0m 
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,121 ColumnFamilyStore.java:427 - Initializing system_distributed.parent_repair_history
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,220 ColumnFamilyStore.java:427 - Initializing system_distributed.repair_history
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,274 ColumnFamilyStore.java:427 - Initializing system_distributed.view_build_status
[34mhive-server                  |[0m [4/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [4/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [4/100] try in 5s once again ...
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,311 ViewManager.java:137 - Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,323 ColumnFamilyStore.java:427 - Initializing system_auth.resource_role_permissons_index
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,341 ColumnFamilyStore.java:427 - Initializing system_auth.role_members
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,407 ColumnFamilyStore.java:427 - Initializing system_auth.role_permissions
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,459 ColumnFamilyStore.java:427 - Initializing system_auth.roles
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,517 ViewManager.java:137 - Not submitting build tasks for views in keyspace system_auth as storage service is not initialized
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,521 ViewManager.java:137 - Not submitting build tasks for views in keyspace feature_store as storage service is not initialized
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,554 ColumnFamilyStore.java:427 - Initializing system_traces.events
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,596 ColumnFamilyStore.java:427 - Initializing system_traces.sessions
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,626 ViewManager.java:137 - Not submitting build tasks for views in keyspace system_traces as storage service is not initialized
[32mfeature_store_cassandra      |[0m INFO  [pool-3-thread-1] 2020-12-04 14:23:07,653 AutoSavingCache.java:174 - Completed loading (15 ms; 12 keys) KeyCache cache
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,692 CommitLog.java:147 - Replaying /opt/cassandra/data/commitlog/CommitLog-6-1607091140448.log
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,794 CommitLog.java:149 - Log replay complete, 0 replayed mutations
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,795 StorageService.java:639 - Populating token metadata from system tables
[35mhive-metastore               |[0m SLF4J: Class path contains multiple SLF4J bindings.
[35mhive-metastore               |[0m SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[35mhive-metastore               |[0m SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[35mhive-metastore               |[0m SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
[35mhive-metastore               |[0m SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:07,871 StorageService.java:646 - Token metadata: Normal Tokens:
[32mfeature_store_cassandra      |[0m /172.21.0.5:[-9222462279526940729, -9217724739638825635, -9151549961426244900, -9136597833165407377, -9115701091624094086, -8925504728471672670, -8857146325389100826, -8836987850072989671, -8805667914205439360, -8801207003684924489, -8730271329515652010, -8711525909978044341, -8663406423891536409, -8572236876347536000, -8570709853748325001, -8569317954359339232, -8558022788179604962, -8555300652773014355, -8525195348617948484, -8517474275393259658, -8381979369603550132, -8323180521874766055, -8140931423667542528, -8041430497125390797, -7900054234908854530, -7852502768627427179, -7843311331735263809, -7735669254629999051, -7611163841763033590, -7573801158441557875, -7520144948494757138, -7497178269007883140, -7449202655839127769, -7431871051889857143, -7334068495991224547, -7221299466298370676, -7134358937110891445, -7067139616167739020, -6907286070419363831, -6749478975951498955, -6699317236253111511, -6610203117406499617, -6558873882574301314, -6512963201491721095, -6489582167941360675, -6431105386388673139, -6309139966879979605, -6286022595309619674, -6248913881862756859, -6227597895151671991, -6193006078463456758, -5937896629784122110, -5913675184845324495, -5768892355720758654, -5733422764469246883, -5694692026270781080, -5662973566293327903, -5577560613354674553, -5548005055494555263, -5531458897555369546, -5452844751628794944, -5444563961520073368, -5356131254839581908, -5301522322223597370, -5300648707360593976, -5263317125206602369, -5101541359620277030, -4964924868531996037, -4904296620831236635, -4831182040615610739, -4558031089111575335, -4540309932640295789, -4537094550598482077, -4259613591415764324, -4220591557700518966, -4063344387153616626, -4035228948985284538, -3907110743192563053, -3896270440661159709, -3819660455371896284, -3810717760295389677, -3706162385958874607, -3641429811498231099, -3562941904943635297, -3501887220289881588, -3490095103987132272, -3433220685152622432, -3381851930603487669, -3263692190134406809, -3225692547268242018, -3165447613019463353, -2958026493938050239, -2918661169886939133, -2899954717261500426, -2849221656213685949, -2846544481065526154, -2703018455337632603, -2699252867322411682, -2646850752393599998, -2588443582297640847, -2529647815633595278, -2495723744160678763, -2358578714082005207, -2323317641372967838, -2113453126667313805, -2089170347268745925, -2058971224502709416, -2056764132194934214, -2009509707125343309, -1904691198252941024, -1803826244366676965, -1783144622509201634, -1655502394787503524, -1612778609706922906, -1557929684011240014, -1414461316693241715, -1403924273843224846, -1090741041435603633, -1031536213055703161, -797576509160914914, -737460129771881139, -715684530501463534, -704983557997436964, -409641939137021197, -337146694316026790, -271257989842786412, -268972032529423155, -257078192892743442, -207564375975841295, -125708361991687768, -77730594681051849, -32601591101317197, 64256610372887892, 121271338954989756, 202919063969201200, 220346142021851285, 251798523943968962, 294116154372933812, 346759538210938881, 365309986693834111, 452696378713007821, 482479979224523474, 554274804072309445, 608766224905863164, 623441826881715785, 665478613716682803, 669226207046755963, 834889313670944771, 1078967558619006540, 1091873580410695655, 1196108293584585512, 1208889646038188607, 1314306528234485519, 1365930770602746197, 1397143853617564888, 1427806758529221982, 1432959612142320372, 1436895272146396613, 1437302186345542681, 1552667446179808613, 1568466223635171804, 1634934012247699851, 1768749434927680281, 1798944430388274733, 1883585726015123908, 1946429773246929562, 1956961488036787702, 2100267329926644454, 2135897883085063014, 2154251332452768347, 2158970341380225745, 2281329506025306846, 2286320340144235835, 2389327987160494988, 2513774485213321164, 2532890183660193514, 2614257510568677222, 2675131356865280632, 2742009889787740058, 2866631975047529270, 2920224300850566680, 3131328396153992652, 3179682579365636815, 3272921229688016692, 3286200332773786314, 3381911677469042705, 3385996052368622493, 3555826281517098791, 3565116237765911230, 3581687187087825683, 3601763283198459940, 3606533710952620163, 3641868505336578708, 3770338870039414139, 3774090906923649163, 3816534254031220279, 3884191877402592795, 3912346151261754763, 3961030711362906997, 3970998512943429386, 4066258062766552273, 4377038866603429513, 4485520094578593345, 4536311093243180967, 4563358932391685744, 4620423090546473801, 4703855626191875945, 4924609019138089882, 4931669496217960705, 4933162400387698133, 5066177946978027916, 5181614186184117871, 5271031424053411441, 5504839486204956921, 5534046281585578369, 5545215022789318862, 5561881892879566269, 5619585619210582096, 5742453635103018975, 5784091183682637065, 6047315214792652256, 6067414848421684736, 6152549059498546168, 6226947915427227435, 6312680169786497716, 6339746999492721257, 6544433811999462475, 6558451757357966186, 6604769750017214952, 6622098249028689607, 6708717723036651221, 6815865468005635224, 6900737124913345813, 6928676410592872879, 7136205081486591229, 7147976497754259490, 7194045954990991964, 7316653396289246944, 7322640315461134561, 7370051386776012029, 7394212976038461610, 7399782971886267427, 7613750723391114203, 7685610640593201240, 8027858181135039677, 8049387002671091191, 8088100529651816314, 8129574354202449035, 8168965524297561585, 8235916769092824128, 8254124585999613166, 8340217050461616189, 8403356175272744790, 8545027220934141032, 9113990167241861408, 9130233083048470412]
[32mfeature_store_cassandra      |[0m 
[35mhive-metastore               |[0m 2020-12-04T14:23:07,925 INFO [main] org.apache.hadoop.hive.conf.HiveConf - Found configuration file file:/opt/hive/conf/hive-site.xml
[36mbroker                       |[0m [2020-12-04 14:23:08,212] INFO KafkaConfig values: 
[36mbroker                       |[0m 	advertised.host.name = null
[36mbroker                       |[0m 	advertised.listeners = PLAINTEXT://broker:9092
[36mbroker                       |[0m 	advertised.port = null
[36mbroker                       |[0m 	alter.config.policy.class.name = null
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	authorizer.class.name = 
[36mbroker                       |[0m 	auto.create.topics.enable = true
[36mbroker                       |[0m 	auto.leader.rebalance.enable = true
[36mbroker                       |[0m 	background.threads = 10
[36mbroker                       |[0m 	broker.id = 1
[36mbroker                       |[0m 	broker.id.generation.enable = true
[36mbroker                       |[0m 	broker.rack = null
[36mbroker                       |[0m 	client.quota.callback.class = null
[36mbroker                       |[0m 	compression.type = producer
[36mbroker                       |[0m 	connection.failed.authentication.delay.ms = 100
[36mbroker                       |[0m 	connections.max.idle.ms = 600000
[36mbroker                       |[0m 	connections.max.reauth.ms = 0
[36mbroker                       |[0m 	control.plane.listener.name = null
[36mbroker                       |[0m 	controlled.shutdown.enable = true
[36mbroker                       |[0m 	controlled.shutdown.max.retries = 3
[36mbroker                       |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mbroker                       |[0m 	controller.socket.timeout.ms = 30000
[36mbroker                       |[0m 	create.topic.policy.class.name = null
[36mbroker                       |[0m 	default.replication.factor = 1
[36mbroker                       |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mbroker                       |[0m 	delegation.token.expiry.time.ms = 86400000
[36mbroker                       |[0m 	delegation.token.master.key = null
[36mbroker                       |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mbroker                       |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mbroker                       |[0m 	delete.topic.enable = true
[36mbroker                       |[0m 	fetch.max.bytes = 57671680
[36mbroker                       |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	group.initial.rebalance.delay.ms = 3000
[36mbroker                       |[0m 	group.max.session.timeout.ms = 1800000
[36mbroker                       |[0m 	group.max.size = 2147483647
[36mbroker                       |[0m 	group.min.session.timeout.ms = 6000
[36mbroker                       |[0m 	host.name = 
[36mbroker                       |[0m 	inter.broker.listener.name = null
[36mbroker                       |[0m 	inter.broker.protocol.version = 2.5-IV0
[36mbroker                       |[0m 	kafka.metrics.polling.interval.secs = 10
[36mbroker                       |[0m 	kafka.metrics.reporters = []
[36mbroker                       |[0m 	leader.imbalance.check.interval.seconds = 300
[36mbroker                       |[0m 	leader.imbalance.per.broker.percentage = 10
[36mbroker                       |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[36mbroker                       |[0m 	listeners = PLAINTEXT://0.0.0.0:9092
[36mbroker                       |[0m 	log.cleaner.backoff.ms = 15000
[36mbroker                       |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mbroker                       |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mbroker                       |[0m 	log.cleaner.enable = true
[36mbroker                       |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mbroker                       |[0m 	log.cleaner.io.buffer.size = 524288
[36mbroker                       |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mbroker                       |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mbroker                       |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mbroker                       |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mbroker                       |[0m 	log.cleaner.threads = 1
[36mbroker                       |[0m 	log.cleanup.policy = [delete]
[36mbroker                       |[0m 	log.dir = /tmp/kafka-logs
[36mbroker                       |[0m 	log.dirs = /var/lib/kafka/data
[36mbroker                       |[0m 	log.flush.interval.messages = 9223372036854775807
[36mbroker                       |[0m 	log.flush.interval.ms = null
[36mbroker                       |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mbroker                       |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.index.interval.bytes = 4096
[36mbroker                       |[0m 	log.index.size.max.bytes = 10485760
[36mbroker                       |[0m 	log.message.downconversion.enable = true
[36mbroker                       |[0m 	log.message.format.version = 2.5-IV0
[36mbroker                       |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mbroker                       |[0m 	log.message.timestamp.type = CreateTime
[36mbroker                       |[0m 	log.preallocate = false
[36mbroker                       |[0m 	log.retention.bytes = -1
[36mbroker                       |[0m 	log.retention.check.interval.ms = 300000
[36mbroker                       |[0m 	log.retention.hours = 168
[36mbroker                       |[0m 	log.retention.minutes = null
[36mbroker                       |[0m 	log.retention.ms = null
[36mbroker                       |[0m 	log.roll.hours = 168
[36mbroker                       |[0m 	log.roll.jitter.hours = 0
[36mbroker                       |[0m 	log.roll.jitter.ms = null
[36mbroker                       |[0m 	log.roll.ms = null
[36mbroker                       |[0m 	log.segment.bytes = 1073741824
[36mbroker                       |[0m 	log.segment.delete.delay.ms = 60000
[36mbroker                       |[0m 	max.connections = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip.overrides = 
[36mbroker                       |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mbroker                       |[0m 	message.max.bytes = 1048588
[36mbroker                       |[0m 	metric.reporters = []
[36mbroker                       |[0m 	metrics.num.samples = 2
[36mbroker                       |[0m 	metrics.recording.level = INFO
[36mbroker                       |[0m 	metrics.sample.window.ms = 30000
[36mbroker                       |[0m 	min.insync.replicas = 1
[36mbroker                       |[0m 	num.io.threads = 8
[36mbroker                       |[0m 	num.network.threads = 3
[36mbroker                       |[0m 	num.partitions = 1
[36mbroker                       |[0m 	num.recovery.threads.per.data.dir = 1
[36mbroker                       |[0m 	num.replica.alter.log.dirs.threads = null
[36mbroker                       |[0m 	num.replica.fetchers = 1
[36mbroker                       |[0m 	offset.metadata.max.bytes = 4096
[36mbroker                       |[0m 	offsets.commit.required.acks = -1
[36mbroker                       |[0m 	offsets.commit.timeout.ms = 5000
[36mbroker                       |[0m 	offsets.load.buffer.size = 5242880
[36mbroker                       |[0m 	offsets.retention.check.interval.ms = 600000
[36mbroker                       |[0m 	offsets.retention.minutes = 10080
[36mbroker                       |[0m 	offsets.topic.compression.codec = 0
[36mbroker                       |[0m 	offsets.topic.num.partitions = 50
[36mbroker                       |[0m 	offsets.topic.replication.factor = 1
[36mbroker                       |[0m 	offsets.topic.segment.bytes = 104857600
[36mbroker                       |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mbroker                       |[0m 	password.encoder.iterations = 4096
[36mbroker                       |[0m 	password.encoder.key.length = 128
[36mbroker                       |[0m 	password.encoder.keyfactory.algorithm = null
[36mbroker                       |[0m 	password.encoder.old.secret = null
[36mbroker                       |[0m 	password.encoder.secret = null
[36mbroker                       |[0m 	port = 9092
[36mbroker                       |[0m 	principal.builder.class = null
[36mbroker                       |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	queued.max.request.bytes = -1
[36mbroker                       |[0m 	queued.max.requests = 500
[36mbroker                       |[0m 	quota.consumer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.producer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.window.num = 11
[36mbroker                       |[0m 	quota.window.size.seconds = 1
[36mbroker                       |[0m 	replica.fetch.backoff.ms = 1000
[36mbroker                       |[0m 	replica.fetch.max.bytes = 1048576
[36mbroker                       |[0m 	replica.fetch.min.bytes = 1
[36mbroker                       |[0m 	replica.fetch.response.max.bytes = 10485760
[36mbroker                       |[0m 	replica.fetch.wait.max.ms = 500
[36mbroker                       |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mbroker                       |[0m 	replica.lag.time.max.ms = 30000
[36mbroker                       |[0m 	replica.selector.class = null
[36mbroker                       |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mbroker                       |[0m 	replica.socket.timeout.ms = 30000
[36mbroker                       |[0m 	replication.quota.window.num = 11
[36mbroker                       |[0m 	replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	request.timeout.ms = 30000
[36mbroker                       |[0m 	reserved.broker.max.id = 1000
[36mbroker                       |[0m 	sasl.client.callback.handler.class = null
[36mbroker                       |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mbroker                       |[0m 	sasl.jaas.config = null
[36mbroker                       |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mbroker                       |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mbroker                       |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mbroker                       |[0m 	sasl.kerberos.service.name = null
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.callback.handler.class = null
[36mbroker                       |[0m 	sasl.login.class = null
[36mbroker                       |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mbroker                       |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mbroker                       |[0m 	sasl.login.refresh.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mbroker                       |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mbroker                       |[0m 	sasl.server.callback.handler.class = null
[36mbroker                       |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mbroker                       |[0m 	security.providers = null
[36mbroker                       |[0m 	socket.receive.buffer.bytes = 102400
[36mbroker                       |[0m 	socket.request.max.bytes = 104857600
[36mbroker                       |[0m 	socket.send.buffer.bytes = 102400
[36mbroker                       |[0m 	ssl.cipher.suites = []
[36mbroker                       |[0m 	ssl.client.auth = none
[36mbroker                       |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mbroker                       |[0m 	ssl.endpoint.identification.algorithm = https
[36mbroker                       |[0m 	ssl.key.password = null
[36mbroker                       |[0m 	ssl.keymanager.algorithm = SunX509
[36mbroker                       |[0m 	ssl.keystore.location = null
[36mbroker                       |[0m 	ssl.keystore.password = null
[36mbroker                       |[0m 	ssl.keystore.type = JKS
[36mbroker                       |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mbroker                       |[0m 	ssl.protocol = TLS
[36mbroker                       |[0m 	ssl.provider = null
[36mbroker                       |[0m 	ssl.secure.random.implementation = null
[36mbroker                       |[0m 	ssl.trustmanager.algorithm = PKIX
[36mbroker                       |[0m 	ssl.truststore.location = null
[36mbroker                       |[0m 	ssl.truststore.password = null
[36mbroker                       |[0m 	ssl.truststore.type = JKS
[36mbroker                       |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mbroker                       |[0m 	transaction.max.timeout.ms = 900000
[36mbroker                       |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mbroker                       |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mbroker                       |[0m 	transaction.state.log.min.isr = 2
[36mbroker                       |[0m 	transaction.state.log.num.partitions = 50
[36mbroker                       |[0m 	transaction.state.log.replication.factor = 3
[36mbroker                       |[0m 	transaction.state.log.segment.bytes = 104857600
[36mbroker                       |[0m 	transactional.id.expiration.ms = 604800000
[36mbroker                       |[0m 	unclean.leader.election.enable = false
[36mbroker                       |[0m 	zookeeper.clientCnxnSocket = null
[36mbroker                       |[0m 	zookeeper.connect = zookeeper:2181
[36mbroker                       |[0m 	zookeeper.connection.timeout.ms = null
[36mbroker                       |[0m 	zookeeper.max.in.flight.requests = 10
[36mbroker                       |[0m 	zookeeper.session.timeout.ms = 18000
[36mbroker                       |[0m 	zookeeper.set.acl = false
[36mbroker                       |[0m 	zookeeper.ssl.cipher.suites = null
[36mbroker                       |[0m 	zookeeper.ssl.client.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.crl.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.enabled.protocols = null
[36mbroker                       |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mbroker                       |[0m 	zookeeper.ssl.keystore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.type = null
[36mbroker                       |[0m 	zookeeper.ssl.ocsp.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mbroker                       |[0m 	zookeeper.ssl.truststore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.type = null
[36mbroker                       |[0m 	zookeeper.sync.time.ms = 2000
[36mbroker                       |[0m  (kafka.server.KafkaConfig)
[36mbroker                       |[0m [2020-12-04 14:23:08,311] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[32mfeature_store_cassandra      |[0m INFO  [Service Thread] 2020-12-04 14:23:08,454 GCInspector.java:285 - ConcurrentMarkSweep GC in 246ms.  Code Cache: 7720384 -> 11936256; Compressed Class Space: 4423608 -> 5063016; Metaspace: 30860600 -> 36089632; Par Eden Space: 6710960 -> 350267264; 
[36mbroker                       |[0m [2020-12-04 14:23:08,491] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[36mbroker                       |[0m [2020-12-04 14:23:08,531] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[36mbroker                       |[0m [2020-12-04 14:23:08,537] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[36mbroker                       |[0m [2020-12-04 14:23:08,539] INFO starting (kafka.server.KafkaServer)
[36mbroker                       |[0m [2020-12-04 14:23:08,542] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mbroker                       |[0m [2020-12-04 14:23:08,625] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[36mbroker                       |[0m [2020-12-04 14:23:08,682] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,684] INFO Client environment:host.name=2fe9a2f7062b (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,684] INFO Client environment:java.version=1.8.0_212 (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,685] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,685] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,685] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/connect-json-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-common-2.28.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.48.Final.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.2.jar:/usr/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/usr/bin/../share/java/kafka/jersey-server-2.28.jar:/usr/bin/../share/java/kafka/connect-api-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/avro-1.9.2.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.48.Final.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-javadoc.jar:/usr/bin/../share/java/kafka/commons-codec-1.11.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.48.Final.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/connect-runtime-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.2.jar:/usr/bin/../share/java/kafka/support-metrics-common-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.2.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-sources.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/scala-library-2.12.10.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-test.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/connect-file-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.2.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/connect-transforms-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.2.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.48.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/usr/bin/../share/java/kafka/jersey-client-2.28.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/support-metrics-client-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/kafka-clients-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/kafka-streams-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.12-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.2.jar:/usr/bin/../share/java/kafka/httpclient-4.5.11.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/commons-compress-1.19.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.48.Final.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-scaladoc.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.2.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.12-2.10.2.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.2.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/netty-common-4.1.48.Final.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-tools-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.48.Final.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/connect-mirror-client-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/kafka_2.12-5.5.1-ccs-test-sources.jar:/usr/bin/../share/java/kafka/httpmime-4.5.11.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.28.jar:/usr/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/usr/bin/../share/java/kafka/javassist-3.22.0-CR2.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/connect-mirror-5.5.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.12/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/* (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,686] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:os.version=5.4.0-56-generic (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,687] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,688] INFO Client environment:os.memory.free=985MB (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,688] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,689] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,710] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@a514af7 (org.apache.zookeeper.ZooKeeper)
[36mbroker                       |[0m [2020-12-04 14:23:08,736] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[36mbroker                       |[0m [2020-12-04 14:23:08,768] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[36mbroker                       |[0m [2020-12-04 14:23:08,835] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[36mbroker                       |[0m [2020-12-04 14:23:08,860] INFO Opening socket connection to server zookeeper/172.21.0.6:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[36mbroker                       |[0m [2020-12-04 14:23:08,871] INFO Socket connection established, initiating session, client: /172.21.0.10:35056, server: zookeeper/172.21.0.6:2181 (org.apache.zookeeper.ClientCnxn)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:08,880 QueryProcessor.java:163 - Preloaded 0 prepared statements
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:08,883 StorageService.java:657 - Cassandra version: 3.11.7
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:08,885 StorageService.java:658 - Thrift API version: 20.1.0
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:08,891 StorageService.java:659 - CQL supported versions: 3.4.4 (default: 3.4.4)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:08,893 StorageService.java:661 - Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4)
[36mbroker                       |[0m [2020-12-04 14:23:08,909] INFO Session establishment complete on server zookeeper/172.21.0.6:2181, sessionid = 0x100021a15d10001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[36mbroker                       |[0m [2020-12-04 14:23:08,919] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,082 IndexSummaryManager.java:87 - Initializing index summary manager with a memory pool size of 244 MB and a resize interval of 60 minutes
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,114 MessagingService.java:750 - Starting Messaging Service on /172.21.0.5:7000 (eth0)
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,265 StorageService.java:589 - Unable to gossip with any peers but continuing anyway since node is in its own seed list
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,293 StorageService.java:743 - Loading persisted ring state
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,300 StorageService.java:871 - Starting up server gossip
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,539 TokenMetadata.java:497 - Updating topology for /172.21.0.5
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,547 TokenMetadata.java:497 - Updating topology for /172.21.0.5
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,704 StorageService.java:1038 - Using saved tokens [-1031536213055703161, -1090741041435603633, -125708361991687768, -1403924273843224846, -1414461316693241715, -1557929684011240014, -1612778609706922906, -1655502394787503524, -1783144622509201634, -1803826244366676965, -1904691198252941024, -2009509707125343309, -2056764132194934214, -2058971224502709416, -207564375975841295, -2089170347268745925, -2113453126667313805, -2323317641372967838, -2358578714082005207, -2495723744160678763, -2529647815633595278, -257078192892743442, -2588443582297640847, -2646850752393599998, -268972032529423155, -2699252867322411682, -2703018455337632603, -271257989842786412, -2846544481065526154, -2849221656213685949, -2899954717261500426, -2918661169886939133, -2958026493938050239, -3165447613019463353, -3225692547268242018, -32601591101317197, -3263692190134406809, -337146694316026790, -3381851930603487669, -3433220685152622432, -3490095103987132272, -3501887220289881588, -3562941904943635297, -3641429811498231099, -3706162385958874607, -3810717760295389677, -3819660455371896284, -3896270440661159709, -3907110743192563053, -4035228948985284538, -4063344387153616626, -409641939137021197, -4220591557700518966, -4259613591415764324, -4537094550598482077, -4540309932640295789, -4558031089111575335, -4831182040615610739, -4904296620831236635, -4964924868531996037, -5101541359620277030, -5263317125206602369, -5300648707360593976, -5301522322223597370, -5356131254839581908, -5444563961520073368, -5452844751628794944, -5531458897555369546, -5548005055494555263, -5577560613354674553, -5662973566293327903, -5694692026270781080, -5733422764469246883, -5768892355720758654, -5913675184845324495, -5937896629784122110, -6193006078463456758, -6227597895151671991, -6248913881862756859, -6286022595309619674, -6309139966879979605, -6431105386388673139, -6489582167941360675, -6512963201491721095, -6558873882574301314, -6610203117406499617, -6699317236253111511, -6749478975951498955, -6907286070419363831, -704983557997436964, -7067139616167739020, -7134358937110891445, -715684530501463534, -7221299466298370676, -7334068495991224547, -737460129771881139, -7431871051889857143, -7449202655839127769, -7497178269007883140, -7520144948494757138, -7573801158441557875, -7611163841763033590, -7735669254629999051, -77730594681051849, -7843311331735263809, -7852502768627427179, -7900054234908854530, -797576509160914914, -8041430497125390797, -8140931423667542528, -8323180521874766055, -8381979369603550132, -8517474275393259658, -8525195348617948484, -8555300652773014355, -8558022788179604962, -8569317954359339232, -8570709853748325001, -8572236876347536000, -8663406423891536409, -8711525909978044341, -8730271329515652010, -8801207003684924489, -8805667914205439360, -8836987850072989671, -8857146325389100826, -8925504728471672670, -9115701091624094086, -9136597833165407377, -9151549961426244900, -9217724739638825635, -9222462279526940729, 1078967558619006540, 1091873580410695655, 1196108293584585512, 1208889646038188607, 121271338954989756, 1314306528234485519, 1365930770602746197, 1397143853617564888, 1427806758529221982, 1432959612142320372, 1436895272146396613, 1437302186345542681, 1552667446179808613, 1568466223635171804, 1634934012247699851, 1768749434927680281, 1798944430388274733, 1883585726015123908, 1946429773246929562, 1956961488036787702, 202919063969201200, 2100267329926644454, 2135897883085063014, 2154251332452768347, 2158970341380225745, 220346142021851285, 2281329506025306846, 2286320340144235835, 2389327987160494988, 2513774485213321164, 251798523943968962, 2532890183660193514, 2614257510568677222, 2675131356865280632, 2742009889787740058, 2866631975047529270, 2920224300850566680, 294116154372933812, 3131328396153992652, 3179682579365636815, 3272921229688016692, 3286200332773786314, 3381911677469042705, 3385996052368622493, 346759538210938881, 3555826281517098791, 3565116237765911230, 3581687187087825683, 3601763283198459940, 3606533710952620163, 3641868505336578708, 365309986693834111, 3770338870039414139, 3774090906923649163, 3816534254031220279, 3884191877402592795, 3912346151261754763, 3961030711362906997, 3970998512943429386, 4066258062766552273, 4377038866603429513, 4485520094578593345, 452696378713007821, 4536311093243180967, 4563358932391685744, 4620423090546473801, 4703855626191875945, 482479979224523474, 4924609019138089882, 4931669496217960705, 4933162400387698133, 5066177946978027916, 5181614186184117871, 5271031424053411441, 5504839486204956921, 5534046281585578369, 554274804072309445, 5545215022789318862, 5561881892879566269, 5619585619210582096, 5742453635103018975, 5784091183682637065, 6047315214792652256, 6067414848421684736, 608766224905863164, 6152549059498546168, 6226947915427227435, 623441826881715785, 6312680169786497716, 6339746999492721257, 64256610372887892, 6544433811999462475, 6558451757357966186, 6604769750017214952, 6622098249028689607, 665478613716682803, 669226207046755963, 6708717723036651221, 6815865468005635224, 6900737124913345813, 6928676410592872879, 7136205081486591229, 7147976497754259490, 7194045954990991964, 7316653396289246944, 7322640315461134561, 7370051386776012029, 7394212976038461610, 7399782971886267427, 7613750723391114203, 7685610640593201240, 8027858181135039677, 8049387002671091191, 8088100529651816314, 8129574354202449035, 8168965524297561585, 8235916769092824128, 8254124585999613166, 8340217050461616189, 834889313670944771, 8403356175272744790, 8545027220934141032, 9113990167241861408, 9130233083048470412]
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,743 StorageService.java:1478 - JOINING: Finish joining ring
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:09,982 StorageService.java:2394 - Node /172.21.0.5 state jump to NORMAL
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:10,006 Gossiper.java:1781 - Waiting for gossip to settle...
[36mbroker                       |[0m [2020-12-04 14:23:10,216] INFO Cluster ID = 1PGv1UMSTLGPjgLbzLo1-g (kafka.server.KafkaServer)
[36mbroker                       |[0m [2020-12-04 14:23:10,433] INFO KafkaConfig values: 
[36mbroker                       |[0m 	advertised.host.name = null
[36mbroker                       |[0m 	advertised.listeners = PLAINTEXT://broker:9092
[36mbroker                       |[0m 	advertised.port = null
[36mbroker                       |[0m 	alter.config.policy.class.name = null
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	authorizer.class.name = 
[36mbroker                       |[0m 	auto.create.topics.enable = true
[36mbroker                       |[0m 	auto.leader.rebalance.enable = true
[36mbroker                       |[0m 	background.threads = 10
[36mbroker                       |[0m 	broker.id = 1
[36mbroker                       |[0m 	broker.id.generation.enable = true
[36mbroker                       |[0m 	broker.rack = null
[36mbroker                       |[0m 	client.quota.callback.class = null
[36mbroker                       |[0m 	compression.type = producer
[36mbroker                       |[0m 	connection.failed.authentication.delay.ms = 100
[36mbroker                       |[0m 	connections.max.idle.ms = 600000
[36mbroker                       |[0m 	connections.max.reauth.ms = 0
[36mbroker                       |[0m 	control.plane.listener.name = null
[36mbroker                       |[0m 	controlled.shutdown.enable = true
[36mbroker                       |[0m 	controlled.shutdown.max.retries = 3
[36mbroker                       |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mbroker                       |[0m 	controller.socket.timeout.ms = 30000
[36mbroker                       |[0m 	create.topic.policy.class.name = null
[36mbroker                       |[0m 	default.replication.factor = 1
[36mbroker                       |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mbroker                       |[0m 	delegation.token.expiry.time.ms = 86400000
[36mbroker                       |[0m 	delegation.token.master.key = null
[36mbroker                       |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mbroker                       |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mbroker                       |[0m 	delete.topic.enable = true
[36mbroker                       |[0m 	fetch.max.bytes = 57671680
[36mbroker                       |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	group.initial.rebalance.delay.ms = 3000
[36mbroker                       |[0m 	group.max.session.timeout.ms = 1800000
[36mbroker                       |[0m 	group.max.size = 2147483647
[36mbroker                       |[0m 	group.min.session.timeout.ms = 6000
[36mbroker                       |[0m 	host.name = 
[36mbroker                       |[0m 	inter.broker.listener.name = null
[36mbroker                       |[0m 	inter.broker.protocol.version = 2.5-IV0
[36mbroker                       |[0m 	kafka.metrics.polling.interval.secs = 10
[36mbroker                       |[0m 	kafka.metrics.reporters = []
[36mbroker                       |[0m 	leader.imbalance.check.interval.seconds = 300
[36mbroker                       |[0m 	leader.imbalance.per.broker.percentage = 10
[36mbroker                       |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[36mbroker                       |[0m 	listeners = PLAINTEXT://0.0.0.0:9092
[36mbroker                       |[0m 	log.cleaner.backoff.ms = 15000
[36mbroker                       |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mbroker                       |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mbroker                       |[0m 	log.cleaner.enable = true
[36mbroker                       |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mbroker                       |[0m 	log.cleaner.io.buffer.size = 524288
[36mbroker                       |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mbroker                       |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mbroker                       |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mbroker                       |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mbroker                       |[0m 	log.cleaner.threads = 1
[36mbroker                       |[0m 	log.cleanup.policy = [delete]
[36mbroker                       |[0m 	log.dir = /tmp/kafka-logs
[36mbroker                       |[0m 	log.dirs = /var/lib/kafka/data
[36mbroker                       |[0m 	log.flush.interval.messages = 9223372036854775807
[36mbroker                       |[0m 	log.flush.interval.ms = null
[36mbroker                       |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mbroker                       |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.index.interval.bytes = 4096
[36mbroker                       |[0m 	log.index.size.max.bytes = 10485760
[36mbroker                       |[0m 	log.message.downconversion.enable = true
[36mbroker                       |[0m 	log.message.format.version = 2.5-IV0
[36mbroker                       |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mbroker                       |[0m 	log.message.timestamp.type = CreateTime
[36mbroker                       |[0m 	log.preallocate = false
[36mbroker                       |[0m 	log.retention.bytes = -1
[36mbroker                       |[0m 	log.retention.check.interval.ms = 300000
[36mbroker                       |[0m 	log.retention.hours = 168
[36mbroker                       |[0m 	log.retention.minutes = null
[36mbroker                       |[0m 	log.retention.ms = null
[36mbroker                       |[0m 	log.roll.hours = 168
[36mbroker                       |[0m 	log.roll.jitter.hours = 0
[36mbroker                       |[0m 	log.roll.jitter.ms = null
[36mbroker                       |[0m 	log.roll.ms = null
[36mbroker                       |[0m 	log.segment.bytes = 1073741824
[36mbroker                       |[0m 	log.segment.delete.delay.ms = 60000
[36mbroker                       |[0m 	max.connections = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip.overrides = 
[36mbroker                       |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mbroker                       |[0m 	message.max.bytes = 1048588
[36mbroker                       |[0m 	metric.reporters = []
[36mbroker                       |[0m 	metrics.num.samples = 2
[36mbroker                       |[0m 	metrics.recording.level = INFO
[36mbroker                       |[0m 	metrics.sample.window.ms = 30000
[36mbroker                       |[0m 	min.insync.replicas = 1
[36mbroker                       |[0m 	num.io.threads = 8
[36mbroker                       |[0m 	num.network.threads = 3
[36mbroker                       |[0m 	num.partitions = 1
[36mbroker                       |[0m 	num.recovery.threads.per.data.dir = 1
[36mbroker                       |[0m 	num.replica.alter.log.dirs.threads = null
[36mbroker                       |[0m 	num.replica.fetchers = 1
[36mbroker                       |[0m 	offset.metadata.max.bytes = 4096
[36mbroker                       |[0m 	offsets.commit.required.acks = -1
[36mbroker                       |[0m 	offsets.commit.timeout.ms = 5000
[36mbroker                       |[0m 	offsets.load.buffer.size = 5242880
[36mbroker                       |[0m 	offsets.retention.check.interval.ms = 600000
[36mbroker                       |[0m 	offsets.retention.minutes = 10080
[36mbroker                       |[0m 	offsets.topic.compression.codec = 0
[36mbroker                       |[0m 	offsets.topic.num.partitions = 50
[36mbroker                       |[0m 	offsets.topic.replication.factor = 1
[36mbroker                       |[0m 	offsets.topic.segment.bytes = 104857600
[36mbroker                       |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mbroker                       |[0m 	password.encoder.iterations = 4096
[36mbroker                       |[0m 	password.encoder.key.length = 128
[36mbroker                       |[0m 	password.encoder.keyfactory.algorithm = null
[36mbroker                       |[0m 	password.encoder.old.secret = null
[36mbroker                       |[0m 	password.encoder.secret = null
[36mbroker                       |[0m 	port = 9092
[36mbroker                       |[0m 	principal.builder.class = null
[36mbroker                       |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	queued.max.request.bytes = -1
[36mbroker                       |[0m 	queued.max.requests = 500
[36mbroker                       |[0m 	quota.consumer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.producer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.window.num = 11
[36mbroker                       |[0m 	quota.window.size.seconds = 1
[36mbroker                       |[0m 	replica.fetch.backoff.ms = 1000
[36mbroker                       |[0m 	replica.fetch.max.bytes = 1048576
[36mbroker                       |[0m 	replica.fetch.min.bytes = 1
[36mbroker                       |[0m 	replica.fetch.response.max.bytes = 10485760
[36mbroker                       |[0m 	replica.fetch.wait.max.ms = 500
[36mbroker                       |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mbroker                       |[0m 	replica.lag.time.max.ms = 30000
[36mbroker                       |[0m 	replica.selector.class = null
[36mbroker                       |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mbroker                       |[0m 	replica.socket.timeout.ms = 30000
[36mbroker                       |[0m 	replication.quota.window.num = 11
[36mbroker                       |[0m 	replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	request.timeout.ms = 30000
[36mbroker                       |[0m 	reserved.broker.max.id = 1000
[36mbroker                       |[0m 	sasl.client.callback.handler.class = null
[36mbroker                       |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mbroker                       |[0m 	sasl.jaas.config = null
[36mbroker                       |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mbroker                       |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mbroker                       |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mbroker                       |[0m 	sasl.kerberos.service.name = null
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.callback.handler.class = null
[36mbroker                       |[0m 	sasl.login.class = null
[36mbroker                       |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mbroker                       |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mbroker                       |[0m 	sasl.login.refresh.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mbroker                       |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mbroker                       |[0m 	sasl.server.callback.handler.class = null
[36mbroker                       |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mbroker                       |[0m 	security.providers = null
[36mbroker                       |[0m 	socket.receive.buffer.bytes = 102400
[36mbroker                       |[0m 	socket.request.max.bytes = 104857600
[36mbroker                       |[0m 	socket.send.buffer.bytes = 102400
[36mbroker                       |[0m 	ssl.cipher.suites = []
[36mbroker                       |[0m 	ssl.client.auth = none
[36mbroker                       |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mbroker                       |[0m 	ssl.endpoint.identification.algorithm = https
[36mbroker                       |[0m 	ssl.key.password = null
[36mbroker                       |[0m 	ssl.keymanager.algorithm = SunX509
[36mbroker                       |[0m 	ssl.keystore.location = null
[36mbroker                       |[0m 	ssl.keystore.password = null
[36mbroker                       |[0m 	ssl.keystore.type = JKS
[36mbroker                       |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mbroker                       |[0m 	ssl.protocol = TLS
[36mbroker                       |[0m 	ssl.provider = null
[36mbroker                       |[0m 	ssl.secure.random.implementation = null
[36mbroker                       |[0m 	ssl.trustmanager.algorithm = PKIX
[36mbroker                       |[0m 	ssl.truststore.location = null
[36mbroker                       |[0m 	ssl.truststore.password = null
[36mbroker                       |[0m 	ssl.truststore.type = JKS
[36mbroker                       |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mbroker                       |[0m 	transaction.max.timeout.ms = 900000
[36mbroker                       |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mbroker                       |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mbroker                       |[0m 	transaction.state.log.min.isr = 2
[36mbroker                       |[0m 	transaction.state.log.num.partitions = 50
[36mbroker                       |[0m 	transaction.state.log.replication.factor = 3
[36mbroker                       |[0m 	transaction.state.log.segment.bytes = 104857600
[36mbroker                       |[0m 	transactional.id.expiration.ms = 604800000
[36mbroker                       |[0m 	unclean.leader.election.enable = false
[36mbroker                       |[0m 	zookeeper.clientCnxnSocket = null
[36mbroker                       |[0m 	zookeeper.connect = zookeeper:2181
[36mbroker                       |[0m 	zookeeper.connection.timeout.ms = null
[36mbroker                       |[0m 	zookeeper.max.in.flight.requests = 10
[36mbroker                       |[0m 	zookeeper.session.timeout.ms = 18000
[36mbroker                       |[0m 	zookeeper.set.acl = false
[36mbroker                       |[0m 	zookeeper.ssl.cipher.suites = null
[36mbroker                       |[0m 	zookeeper.ssl.client.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.crl.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.enabled.protocols = null
[36mbroker                       |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mbroker                       |[0m 	zookeeper.ssl.keystore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.type = null
[36mbroker                       |[0m 	zookeeper.ssl.ocsp.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mbroker                       |[0m 	zookeeper.ssl.truststore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.type = null
[36mbroker                       |[0m 	zookeeper.sync.time.ms = 2000
[36mbroker                       |[0m  (kafka.server.KafkaConfig)
[36mbroker                       |[0m [2020-12-04 14:23:10,483] INFO KafkaConfig values: 
[36mbroker                       |[0m 	advertised.host.name = null
[36mbroker                       |[0m 	advertised.listeners = PLAINTEXT://broker:9092
[36mbroker                       |[0m 	advertised.port = null
[36mbroker                       |[0m 	alter.config.policy.class.name = null
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mbroker                       |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	authorizer.class.name = 
[36mbroker                       |[0m 	auto.create.topics.enable = true
[36mbroker                       |[0m 	auto.leader.rebalance.enable = true
[36mbroker                       |[0m 	background.threads = 10
[36mbroker                       |[0m 	broker.id = 1
[36mbroker                       |[0m 	broker.id.generation.enable = true
[36mbroker                       |[0m 	broker.rack = null
[36mbroker                       |[0m 	client.quota.callback.class = null
[36mbroker                       |[0m 	compression.type = producer
[36mbroker                       |[0m 	connection.failed.authentication.delay.ms = 100
[36mbroker                       |[0m 	connections.max.idle.ms = 600000
[36mbroker                       |[0m 	connections.max.reauth.ms = 0
[36mbroker                       |[0m 	control.plane.listener.name = null
[36mbroker                       |[0m 	controlled.shutdown.enable = true
[36mbroker                       |[0m 	controlled.shutdown.max.retries = 3
[36mbroker                       |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mbroker                       |[0m 	controller.socket.timeout.ms = 30000
[36mbroker                       |[0m 	create.topic.policy.class.name = null
[36mbroker                       |[0m 	default.replication.factor = 1
[36mbroker                       |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mbroker                       |[0m 	delegation.token.expiry.time.ms = 86400000
[36mbroker                       |[0m 	delegation.token.master.key = null
[36mbroker                       |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mbroker                       |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mbroker                       |[0m 	delete.topic.enable = true
[36mbroker                       |[0m 	fetch.max.bytes = 57671680
[36mbroker                       |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	group.initial.rebalance.delay.ms = 3000
[36mbroker                       |[0m 	group.max.session.timeout.ms = 1800000
[36mbroker                       |[0m 	group.max.size = 2147483647
[36mbroker                       |[0m 	group.min.session.timeout.ms = 6000
[36mbroker                       |[0m 	host.name = 
[36mbroker                       |[0m 	inter.broker.listener.name = null
[36mbroker                       |[0m 	inter.broker.protocol.version = 2.5-IV0
[36mbroker                       |[0m 	kafka.metrics.polling.interval.secs = 10
[36mbroker                       |[0m 	kafka.metrics.reporters = []
[36mbroker                       |[0m 	leader.imbalance.check.interval.seconds = 300
[36mbroker                       |[0m 	leader.imbalance.per.broker.percentage = 10
[36mbroker                       |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[36mbroker                       |[0m 	listeners = PLAINTEXT://0.0.0.0:9092
[36mbroker                       |[0m 	log.cleaner.backoff.ms = 15000
[36mbroker                       |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mbroker                       |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mbroker                       |[0m 	log.cleaner.enable = true
[36mbroker                       |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mbroker                       |[0m 	log.cleaner.io.buffer.size = 524288
[36mbroker                       |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mbroker                       |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mbroker                       |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mbroker                       |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mbroker                       |[0m 	log.cleaner.threads = 1
[36mbroker                       |[0m 	log.cleanup.policy = [delete]
[36mbroker                       |[0m 	log.dir = /tmp/kafka-logs
[36mbroker                       |[0m 	log.dirs = /var/lib/kafka/data
[36mbroker                       |[0m 	log.flush.interval.messages = 9223372036854775807
[36mbroker                       |[0m 	log.flush.interval.ms = null
[36mbroker                       |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mbroker                       |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mbroker                       |[0m 	log.index.interval.bytes = 4096
[36mbroker                       |[0m 	log.index.size.max.bytes = 10485760
[36mbroker                       |[0m 	log.message.downconversion.enable = true
[36mbroker                       |[0m 	log.message.format.version = 2.5-IV0
[36mbroker                       |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mbroker                       |[0m 	log.message.timestamp.type = CreateTime
[36mbroker                       |[0m 	log.preallocate = false
[36mbroker                       |[0m 	log.retention.bytes = -1
[36mbroker                       |[0m 	log.retention.check.interval.ms = 300000
[36mbroker                       |[0m 	log.retention.hours = 168
[36mbroker                       |[0m 	log.retention.minutes = null
[36mbroker                       |[0m 	log.retention.ms = null
[36mbroker                       |[0m 	log.roll.hours = 168
[36mbroker                       |[0m 	log.roll.jitter.hours = 0
[36mbroker                       |[0m 	log.roll.jitter.ms = null
[36mbroker                       |[0m 	log.roll.ms = null
[36mbroker                       |[0m 	log.segment.bytes = 1073741824
[36mbroker                       |[0m 	log.segment.delete.delay.ms = 60000
[36mbroker                       |[0m 	max.connections = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip = 2147483647
[36mbroker                       |[0m 	max.connections.per.ip.overrides = 
[36mbroker                       |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mbroker                       |[0m 	message.max.bytes = 1048588
[36mbroker                       |[0m 	metric.reporters = []
[36mbroker                       |[0m 	metrics.num.samples = 2
[36mbroker                       |[0m 	metrics.recording.level = INFO
[36mbroker                       |[0m 	metrics.sample.window.ms = 30000
[36mbroker                       |[0m 	min.insync.replicas = 1
[36mbroker                       |[0m 	num.io.threads = 8
[36mbroker                       |[0m 	num.network.threads = 3
[36mbroker                       |[0m 	num.partitions = 1
[36mbroker                       |[0m 	num.recovery.threads.per.data.dir = 1
[36mbroker                       |[0m 	num.replica.alter.log.dirs.threads = null
[36mbroker                       |[0m 	num.replica.fetchers = 1
[36mbroker                       |[0m 	offset.metadata.max.bytes = 4096
[36mbroker                       |[0m 	offsets.commit.required.acks = -1
[36mbroker                       |[0m 	offsets.commit.timeout.ms = 5000
[36mbroker                       |[0m 	offsets.load.buffer.size = 5242880
[36mbroker                       |[0m 	offsets.retention.check.interval.ms = 600000
[36mbroker                       |[0m 	offsets.retention.minutes = 10080
[36mbroker                       |[0m 	offsets.topic.compression.codec = 0
[36mbroker                       |[0m 	offsets.topic.num.partitions = 50
[36mbroker                       |[0m 	offsets.topic.replication.factor = 1
[36mbroker                       |[0m 	offsets.topic.segment.bytes = 104857600
[36mbroker                       |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mbroker                       |[0m 	password.encoder.iterations = 4096
[36mbroker                       |[0m 	password.encoder.key.length = 128
[36mbroker                       |[0m 	password.encoder.keyfactory.algorithm = null
[36mbroker                       |[0m 	password.encoder.old.secret = null
[36mbroker                       |[0m 	password.encoder.secret = null
[36mbroker                       |[0m 	port = 9092
[36mbroker                       |[0m 	principal.builder.class = null
[36mbroker                       |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mbroker                       |[0m 	queued.max.request.bytes = -1
[36mbroker                       |[0m 	queued.max.requests = 500
[36mbroker                       |[0m 	quota.consumer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.producer.default = 9223372036854775807
[36mbroker                       |[0m 	quota.window.num = 11
[36mbroker                       |[0m 	quota.window.size.seconds = 1
[36mbroker                       |[0m 	replica.fetch.backoff.ms = 1000
[36mbroker                       |[0m 	replica.fetch.max.bytes = 1048576
[36mbroker                       |[0m 	replica.fetch.min.bytes = 1
[36mbroker                       |[0m 	replica.fetch.response.max.bytes = 10485760
[36mbroker                       |[0m 	replica.fetch.wait.max.ms = 500
[36mbroker                       |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mbroker                       |[0m 	replica.lag.time.max.ms = 30000
[36mbroker                       |[0m 	replica.selector.class = null
[36mbroker                       |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mbroker                       |[0m 	replica.socket.timeout.ms = 30000
[36mbroker                       |[0m 	replication.quota.window.num = 11
[36mbroker                       |[0m 	replication.quota.window.size.seconds = 1
[36mbroker                       |[0m 	request.timeout.ms = 30000
[36mbroker                       |[0m 	reserved.broker.max.id = 1000
[36mbroker                       |[0m 	sasl.client.callback.handler.class = null
[36mbroker                       |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mbroker                       |[0m 	sasl.jaas.config = null
[36mbroker                       |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mbroker                       |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mbroker                       |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mbroker                       |[0m 	sasl.kerberos.service.name = null
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.callback.handler.class = null
[36mbroker                       |[0m 	sasl.login.class = null
[36mbroker                       |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mbroker                       |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mbroker                       |[0m 	sasl.login.refresh.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mbroker                       |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mbroker                       |[0m 	sasl.server.callback.handler.class = null
[36mbroker                       |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mbroker                       |[0m 	security.providers = null
[36mbroker                       |[0m 	socket.receive.buffer.bytes = 102400
[36mbroker                       |[0m 	socket.request.max.bytes = 104857600
[36mbroker                       |[0m 	socket.send.buffer.bytes = 102400
[36mbroker                       |[0m 	ssl.cipher.suites = []
[36mbroker                       |[0m 	ssl.client.auth = none
[36mbroker                       |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mbroker                       |[0m 	ssl.endpoint.identification.algorithm = https
[36mbroker                       |[0m 	ssl.key.password = null
[36mbroker                       |[0m 	ssl.keymanager.algorithm = SunX509
[36mbroker                       |[0m 	ssl.keystore.location = null
[36mbroker                       |[0m 	ssl.keystore.password = null
[36mbroker                       |[0m 	ssl.keystore.type = JKS
[36mbroker                       |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mbroker                       |[0m 	ssl.protocol = TLS
[36mbroker                       |[0m 	ssl.provider = null
[36mbroker                       |[0m 	ssl.secure.random.implementation = null
[36mbroker                       |[0m 	ssl.trustmanager.algorithm = PKIX
[36mbroker                       |[0m 	ssl.truststore.location = null
[36mbroker                       |[0m 	ssl.truststore.password = null
[36mbroker                       |[0m 	ssl.truststore.type = JKS
[36mbroker                       |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mbroker                       |[0m 	transaction.max.timeout.ms = 900000
[36mbroker                       |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mbroker                       |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mbroker                       |[0m 	transaction.state.log.min.isr = 2
[36mbroker                       |[0m 	transaction.state.log.num.partitions = 50
[36mbroker                       |[0m 	transaction.state.log.replication.factor = 3
[36mbroker                       |[0m 	transaction.state.log.segment.bytes = 104857600
[36mbroker                       |[0m 	transactional.id.expiration.ms = 604800000
[36mbroker                       |[0m 	unclean.leader.election.enable = false
[36mbroker                       |[0m 	zookeeper.clientCnxnSocket = null
[36mbroker                       |[0m 	zookeeper.connect = zookeeper:2181
[36mbroker                       |[0m 	zookeeper.connection.timeout.ms = null
[36mbroker                       |[0m 	zookeeper.max.in.flight.requests = 10
[36mbroker                       |[0m 	zookeeper.session.timeout.ms = 18000
[36mbroker                       |[0m 	zookeeper.set.acl = false
[36mbroker                       |[0m 	zookeeper.ssl.cipher.suites = null
[36mbroker                       |[0m 	zookeeper.ssl.client.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.crl.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.enabled.protocols = null
[36mbroker                       |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mbroker                       |[0m 	zookeeper.ssl.keystore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.keystore.type = null
[36mbroker                       |[0m 	zookeeper.ssl.ocsp.enable = false
[36mbroker                       |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mbroker                       |[0m 	zookeeper.ssl.truststore.location = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.password = null
[36mbroker                       |[0m 	zookeeper.ssl.truststore.type = null
[36mbroker                       |[0m 	zookeeper.sync.time.ms = 2000
[36mbroker                       |[0m  (kafka.server.KafkaConfig)
[36mbroker                       |[0m [2020-12-04 14:23:10,572] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mbroker                       |[0m [2020-12-04 14:23:10,572] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mbroker                       |[0m [2020-12-04 14:23:10,578] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mbroker                       |[0m [2020-12-04 14:23:10,677] INFO Loading logs. (kafka.log.LogManager)
[36mbroker                       |[0m [2020-12-04 14:23:10,890] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 5 with message format version 2 (kafka.log.Log)
[36mbroker                       |[0m [2020-12-04 14:23:10,927] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Loading producer state from snapshot file '/var/lib/kafka/data/__confluent.support.metrics-0/00000000000000000005.snapshot' (kafka.log.ProducerStateManager)
[36mbroker                       |[0m [2020-12-04 14:23:10,971] INFO [Log partition=__confluent.support.metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset 0 and log end offset 5 in 211 ms (kafka.log.Log)
[36mbroker                       |[0m [2020-12-04 14:23:11,004] INFO Logs loading complete in 326 ms. (kafka.log.LogManager)
[36mbroker                       |[0m [2020-12-04 14:23:11,046] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[36mbroker                       |[0m [2020-12-04 14:23:11,051] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[36mbroker                       |[0m [2020-12-04 14:23:11,065] INFO Starting the log cleaner (kafka.log.LogCleaner)
[36mbroker                       |[0m [2020-12-04 14:23:11,210] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[36mbroker                       |[0m [2020-12-04 14:23:12,011] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[36mbroker                       |[0m [2020-12-04 14:23:12,104] INFO [SocketServer brokerId=1] Created data-plane acceptor and processors for endpoint : EndPoint(0.0.0.0,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[36mbroker                       |[0m [2020-12-04 14:23:12,107] INFO [SocketServer brokerId=1] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[36mbroker                       |[0m [2020-12-04 14:23:12,145] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,146] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,149] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,150] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,187] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[34mhive-server                  |[0m [5/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [5/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [5/100] try in 5s once again ...
[36mbroker                       |[0m [2020-12-04 14:23:12,325] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[36mbroker                       |[0m [2020-12-04 14:23:12,367] INFO Stat of the created znode at /brokers/ids/1 is: 156,156,1607091792349,1607091792349,1,0,0,72059905096351745,182,0,156
[36mbroker                       |[0m  (kafka.zk.KafkaZkClient)
[36mbroker                       |[0m [2020-12-04 14:23:12,369] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(broker,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 156 (kafka.zk.KafkaZkClient)
[36mbroker                       |[0m [2020-12-04 14:23:12,498] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mbroker                       |[0m [2020-12-04 14:23:12,510] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,517] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,524] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mbroker                       |[0m [2020-12-04 14:23:12,565] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 6 and epoch zk version is now 6 (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,567] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,576] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,585] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,592] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,600] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[36mbroker                       |[0m [2020-12-04 14:23:12,612] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[36mbroker                       |[0m [2020-12-04 14:23:12,627] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 13 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mbroker                       |[0m [2020-12-04 14:23:12,636] INFO [Controller id=1] Initialized broker epochs cache: Map(1 -> 156) (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,650] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:5000,blockEndProducerId:5999) by writing to Zk with path version 6 (kafka.coordinator.transaction.ProducerIdManager)
[36mbroker                       |[0m [2020-12-04 14:23:12,653] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,684] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
[36mbroker                       |[0m [2020-12-04 14:23:12,758] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,759] INFO [Controller id=1] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,760] INFO [Controller id=1] Current list of topics in the cluster: Set(__confluent.support.metrics) (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,761] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,771] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
[36mbroker                       |[0m [2020-12-04 14:23:12,774] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[36mbroker                       |[0m [2020-12-04 14:23:12,779] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,779] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36mbroker                       |[0m [2020-12-04 14:23:12,780] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,786] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mbroker                       |[0m [2020-12-04 14:23:12,791] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,800] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set() (kafka.controller.TopicDeletionManager)
[36mbroker                       |[0m [2020-12-04 14:23:12,803] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:12,842] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
[35mhive-metastore               |[0m 2020-12-04T14:23:12,836 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - STARTUP_MSG: 
[35mhive-metastore               |[0m /************************************************************
[35mhive-metastore               |[0m STARTUP_MSG: Starting HiveMetaStore
[35mhive-metastore               |[0m STARTUP_MSG:   host = ef65e30fc5ca/172.21.0.7
[35mhive-metastore               |[0m STARTUP_MSG:   args = []
[35mhive-metastore               |[0m STARTUP_MSG:   version = 2.3.2
[35mhive-metastore               |[0m STARTUP_MSG:   classpath = /opt/hive/conf:/opt/hive/lib/HikariCP-2.5.1.jar:/opt/hive/lib/RoaringBitmap-0.5.18.jar:/opt/hive/lib/ST4-4.0.4.jar:/opt/hive/lib/accumulo-core-1.6.0.jar:/opt/hive/lib/accumulo-fate-1.6.0.jar:/opt/hive/lib/accumulo-start-1.6.0.jar:/opt/hive/lib/accumulo-trace-1.6.0.jar:/opt/hive/lib/activation-1.1.jar:/opt/hive/lib/aether-api-0.9.0.M2.jar:/opt/hive/lib/aether-connector-file-0.9.0.M2.jar:/opt/hive/lib/aether-connector-okhttp-0.0.9.jar:/opt/hive/lib/aether-impl-0.9.0.M2.jar:/opt/hive/lib/aether-spi-0.9.0.M2.jar:/opt/hive/lib/aether-util-0.9.0.M2.jar:/opt/hive/lib/aircompressor-0.3.jar:/opt/hive/lib/airline-0.7.jar:/opt/hive/lib/ant-1.6.5.jar:/opt/hive/lib/ant-1.9.1.jar:/opt/hive/lib/ant-launcher-1.9.1.jar:/opt/hive/lib/antlr-runtime-3.5.2.jar:/opt/hive/lib/antlr4-runtime-4.5.jar:/opt/hive/lib/asm-3.1.jar:/opt/hive/lib/asm-commons-3.1.jar:/opt/hive/lib/asm-tree-3.1.jar:/opt/hive/lib/avatica-1.8.0.jar:/opt/hive/lib/avatica-metrics-1.8.0.jar:/opt/hive/lib/avro-1.7.7.jar:/opt/hive/lib/bonecp-0.8.0.RELEASE.jar:/opt/hive/lib/bytebuffer-collections-0.2.5.jar:/opt/hive/lib/calcite-core-1.10.0.jar:/opt/hive/lib/calcite-druid-1.10.0.jar:/opt/hive/lib/calcite-linq4j-1.10.0.jar:/opt/hive/lib/classmate-1.0.0.jar:/opt/hive/lib/commons-cli-1.2.jar:/opt/hive/lib/commons-codec-1.4.jar:/opt/hive/lib/commons-collections-3.2.2.jar:/opt/hive/lib/commons-compiler-2.7.6.jar:/opt/hive/lib/commons-compress-1.9.jar:/opt/hive/lib/commons-dbcp-1.4.jar:/opt/hive/lib/commons-dbcp2-2.0.1.jar:/opt/hive/lib/commons-el-1.0.jar:/opt/hive/lib/commons-httpclient-3.0.1.jar:/opt/hive/lib/commons-io-2.4.jar:/opt/hive/lib/commons-lang-2.6.jar:/opt/hive/lib/commons-lang3-3.1.jar:/opt/hive/lib/commons-logging-1.2.jar:/opt/hive/lib/commons-math-2.2.jar:/opt/hive/lib/commons-math3-3.6.1.jar:/opt/hive/lib/commons-pool-1.5.4.jar:/opt/hive/lib/commons-pool2-2.2.jar:/opt/hive/lib/commons-vfs2-2.0.jar:/opt/hive/lib/compress-lzf-1.0.3.jar:/opt/hive/lib/config-magic-0.9.jar:/opt/hive/lib/curator-client-2.7.1.jar:/opt/hive/lib/curator-framework-2.7.1.jar:/opt/hive/lib/curator-recipes-2.7.1.jar:/opt/hive/lib/curator-x-discovery-2.11.0.jar:/opt/hive/lib/datanucleus-api-jdo-4.2.4.jar:/opt/hive/lib/datanucleus-core-4.1.17.jar:/opt/hive/lib/datanucleus-rdbms-4.1.19.jar:/opt/hive/lib/derby-10.10.2.0.jar:/opt/hive/lib/derbyclient-10.11.1.1.jar:/opt/hive/lib/derbynet-10.11.1.1.jar:/opt/hive/lib/disruptor-3.3.0.jar:/opt/hive/lib/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/opt/hive/lib/druid-api-0.9.2.jar:/opt/hive/lib/druid-common-0.9.2.jar:/opt/hive/lib/druid-console-0.0.2.jar:/opt/hive/lib/druid-hdfs-storage-0.9.2.jar:/opt/hive/lib/druid-processing-0.9.2.jar:/opt/hive/lib/druid-server-0.9.2.jar:/opt/hive/lib/eigenbase-properties-1.1.5.jar:/opt/hive/lib/emitter-0.3.6.jar:/opt/hive/lib/extendedset-1.3.10.jar:/opt/hive/lib/findbugs-annotations-1.3.9-1.jar:/opt/hive/lib/geoip2-0.4.0.jar:/opt/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:/opt/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:/opt/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:/opt/hive/lib/google-http-client-jackson2-1.15.0-rc.jar:/opt/hive/lib/groovy-all-2.4.4.jar:/opt/hive/lib/gson-2.2.4.jar:/opt/hive/lib/guava-14.0.1.jar:/opt/hive/lib/guice-multibindings-4.1.0.jar:/opt/hive/lib/guice-servlet-4.1.0.jar:/opt/hive/lib/hbase-annotations-1.1.1.jar:/opt/hive/lib/hbase-client-1.1.1.jar:/opt/hive/lib/hbase-common-1.1.1-tests.jar:/opt/hive/lib/hbase-common-1.1.1.jar:/opt/hive/lib/hbase-hadoop-compat-1.1.1.jar:/opt/hive/lib/hbase-hadoop2-compat-1.1.1-tests.jar:/opt/hive/lib/hbase-hadoop2-compat-1.1.1.jar:/opt/hive/lib/hbase-prefix-tree-1.1.1.jar:/opt/hive/lib/hbase-procedure-1.1.1.jar:/opt/hive/lib/hbase-protocol-1.1.1.jar:/opt/hive/lib/hbase-server-1.1.1.jar:/opt/hive/lib/hibernate-validator-5.1.3.Final.jar:/opt/hive/lib/hive-accumulo-handler-2.3.2.jar:/opt/hive/lib/hive-beeline-2.3.2.jar:/opt/hive/lib/hive-cli-2.3.2.jar:/opt/hive/lib/hive-common-2.3.2.jar:/opt/hive/lib/hive-contrib-2.3.2.jar:/opt/hive/lib/hive-druid-handler-2.3.2.jar:/opt/hive/lib/hive-exec-2.3.2.jar:/opt/hive/lib/hive-hbase-handler-2.3.2.jar:/opt/hive/lib/hive-hcatalog-core-2.3.2.jar:/opt/hive/lib/hive-hcatalog-server-extensions-2.3.2.jar:/opt/hive/lib/hive-hplsql-2.3.2.jar:/opt/hive/lib/hive-jdbc-2.3.2.jar:/opt/hive/lib/hive-jdbc-handler-2.3.2.jar:/opt/hive/lib/hive-llap-client-2.3.2.jar:/opt/hive/lib/hive-llap-common-2.3.2-tests.jar:/opt/hive/lib/hive-llap-common-2.3.2.jar:/opt/hive/lib/hive-llap-ext-client-2.3.2.jar:/opt/hive/lib/hive-llap-server-2.3.2.jar:/opt/hive/lib/hive-llap-tez-2.3.2.jar:/opt/hive/lib/hive-metastore-2.3.2.jar:/opt/hive/lib/hive-serde-2.3.2.jar:/opt/hive/lib/hive-service-2.3.2.jar:/opt/hive/lib/hive-service-rpc-2.3.2.jar:/opt/hive/lib/hive-shims-0.23-2.3.2.jar:/opt/hive/lib/hive-shims-2.3.2.jar:/opt/hive/lib/hive-shims-common-2.3.2.jar:/opt/hive/lib/hive-shims-scheduler-2.3.2.jar:/opt/hive/lib/hive-storage-api-2.4.0.jar:/opt/hive/lib/hive-testutils-2.3.2.jar:/opt/hive/lib/hive-vector-code-gen-2.3.2.jar:/opt/hive/lib/htrace-core-3.1.0-incubating.jar:/opt/hive/lib/http-client-1.0.4.jar:/opt/hive/lib/httpclient-4.4.jar:/opt/hive/lib/httpcore-4.4.jar:/opt/hive/lib/icu4j-4.8.1.jar:/opt/hive/lib/irc-api-1.0-0014.jar:/opt/hive/lib/ivy-2.4.0.jar:/opt/hive/lib/jackson-annotations-2.6.0.jar:/opt/hive/lib/jackson-core-2.6.5.jar:/opt/hive/lib/jackson-databind-2.6.5.jar:/opt/hive/lib/jackson-dataformat-smile-2.4.6.jar:/opt/hive/lib/jackson-datatype-guava-2.4.6.jar:/opt/hive/lib/jackson-datatype-joda-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-1.9.13.jar:/opt/hive/lib/jackson-jaxrs-base-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-json-provider-2.4.6.jar:/opt/hive/lib/jackson-jaxrs-smile-provider-2.4.6.jar:/opt/hive/lib/jackson-module-jaxb-annotations-2.4.6.jar:/opt/hive/lib/jackson-xc-1.9.13.jar:/opt/hive/lib/jamon-runtime-2.3.1.jar:/opt/hive/lib/janino-2.7.6.jar:/opt/hive/lib/jasper-compiler-5.5.23.jar:/opt/hive/lib/jasper-runtime-5.5.23.jar:/opt/hive/lib/java-util-0.27.10.jar:/opt/hive/lib/javax.el-3.0.0.jar:/opt/hive/lib/javax.el-api-3.0.0.jar:/opt/hive/lib/javax.inject-1.jar:/opt/hive/lib/javax.jdo-3.2.0-m3.jar:/opt/hive/lib/javax.servlet-3.0.0.v201112011016.jar:/opt/hive/lib/javax.servlet-api-3.1.0.jar:/opt/hive/lib/javolution-5.5.1.jar:/opt/hive/lib/jboss-logging-3.1.3.GA.jar:/opt/hive/lib/jcodings-1.0.8.jar:/opt/hive/lib/jcommander-1.32.jar:/opt/hive/lib/jdbi-2.63.1.jar:/opt/hive/lib/jdo-api-3.0.1.jar:/opt/hive/lib/jersey-client-1.9.jar:/opt/hive/lib/jersey-guice-1.19.jar:/opt/hive/lib/jersey-server-1.14.jar:/opt/hive/lib/jettison-1.1.jar:/opt/hive/lib/jetty-6.1.26.jar:/opt/hive/lib/jetty-all-7.6.0.v20120127.jar:/opt/hive/lib/jetty-client-9.2.5.v20141112.jar:/opt/hive/lib/jetty-continuation-9.2.5.v20141112.jar:/opt/hive/lib/jetty-http-9.2.5.v20141112.jar:/opt/hive/lib/jetty-io-9.2.5.v20141112.jar:/opt/hive/lib/jetty-proxy-9.2.5.v20141112.jar:/opt/hive/lib/jetty-security-9.2.5.v20141112.jar:/opt/hive/lib/jetty-server-9.2.5.v20141112.jar:/opt/hive/lib/jetty-servlet-9.2.5.v20141112.jar:/opt/hive/lib/jetty-servlets-9.2.5.v20141112.jar:/opt/hive/lib/jetty-sslengine-6.1.26.jar:/opt/hive/lib/jetty-util-6.1.26.jar:/opt/hive/lib/jetty-util-9.2.5.v20141112.jar:/opt/hive/lib/jline-2.12.jar:/opt/hive/lib/joda-time-2.8.1.jar:/opt/hive/lib/joni-2.1.2.jar:/opt/hive/lib/jpam-1.1.jar:/opt/hive/lib/json-1.8.jar:/opt/hive/lib/json-path-2.1.0.jar:/opt/hive/lib/jsp-2.1-6.1.14.jar:/opt/hive/lib/jsp-api-2.0.jar:/opt/hive/lib/jsp-api-2.1-6.1.14.jar:/opt/hive/lib/jsp-api-2.1.jar:/opt/hive/lib/jsr305-3.0.0.jar:/opt/hive/lib/jta-1.1.jar:/opt/hive/lib/libfb303-0.9.3.jar:/opt/hive/lib/libthrift-0.9.3.jar:/opt/hive/lib/log4j-1.2-api-2.6.2.jar:/opt/hive/lib/log4j-api-2.6.2.jar:/opt/hive/lib/log4j-core-2.6.2.jar:/opt/hive/lib/log4j-jul-2.5.jar:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar:/opt/hive/lib/log4j-web-2.6.2.jar:/opt/hive/lib/lz4-1.3.0.jar:/opt/hive/lib/mail-1.4.1.jar:/opt/hive/lib/mapdb-1.0.8.jar:/opt/hive/lib/maven-aether-provider-3.1.1.jar:/opt/hive/lib/maven-model-3.1.1.jar:/opt/hive/lib/maven-model-builder-3.1.1.jar:/opt/hive/lib/maven-repository-metadata-3.1.1.jar:/opt/hive/lib/maven-scm-api-1.4.jar:/opt/hive/lib/maven-scm-provider-svn-commons-1.4.jar:/opt/hive/lib/maven-scm-provider-svnexe-1.4.jar:/opt/hive/lib/maven-settings-3.1.1.jar:/opt/hive/lib/maven-settings-builder-3.1.1.jar:/opt/hive/lib/maxminddb-0.2.0.jar:/opt/hive/lib/metrics-core-2.2.0.jar:/opt/hive/lib/metrics-core-3.1.0.jar:/opt/hive/lib/metrics-json-3.1.0.jar:/opt/hive/lib/metrics-jvm-3.1.0.jar:/opt/hive/lib/mysql-metadata-storage-0.9.2.jar:/opt/hive/lib/netty-3.6.2.Final.jar:/opt/hive/lib/netty-all-4.0.52.Final.jar:/opt/hive/lib/okhttp-1.0.2.jar:/opt/hive/lib/opencsv-2.3.jar:/opt/hive/lib/orc-core-1.3.3.jar:/opt/hive/lib/org.abego.treelayout.core-1.0.1.jar:/opt/hive/lib/paranamer-2.3.jar:/opt/hive/lib/parquet-hadoop-bundle-1.8.1.jar:/opt/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/opt/hive/lib/plexus-interpolation-1.19.jar:/opt/hive/lib/plexus-utils-3.0.15.jar:/opt/hive/lib/postgresql-9.4.1208.jre7.jar:/opt/hive/lib/postgresql-jdbc.jar:/opt/hive/lib/postgresql-metadata-storage-0.9.2.jar:/opt/hive/lib/protobuf-java-2.5.0.jar:/opt/hive/lib/regexp-1.3.jar:/opt/hive/lib/rhino-1.7R5.jar:/opt/hive/lib/server-metrics-0.2.8.jar:/opt/hive/lib/servlet-api-2.4.jar:/opt/hive/lib/servlet-api-2.5-6.1.14.jar:/opt/hive/lib/slider-core-0.90.2-incubating.jar:/opt/hive/lib/snappy-java-1.0.5.jar:/opt/hive/lib/spymemcached-2.11.7.jar:/opt/hive/lib/stax-api-1.0.1.jar:/opt/hive/lib/super-csv-2.2.0.jar:/opt/hive/lib/tempus-fugit-1.1.jar:/opt/hive/lib/tesla-aether-0.0.5.jar:/opt/hive/lib/transaction-api-1.1.jar:/opt/hive/lib/validation-api-1.1.0.Final.jar:/opt/hive/lib/velocity-1.5.jar:/opt/hive/lib/wagon-provider-api-2.4.jar:/opt/hive/lib/zookeeper-3.4.6.jar::/opt/hadoop-2.7.4/share/hadoop/tools/lib/hadoop-distcp-2.7.4.jar:/opt/hadoop-2.7.4/contrib/capacity-scheduler/*.jar:/etc/hadoop:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/opt/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar
[35mhive-metastore               |[0m STARTUP_MSG:   build = git://stakiar-MBP.local/Users/stakiar/Desktop/scratch-space/apache-hive -r 857a9fd8ad725a53bd95c1b2d6612f9b1155f44d; compiled by 'stakiar' on Thu Nov 9 09:11:39 PST 2017
[35mhive-metastore               |[0m ************************************************************/
[36mbroker                       |[0m [2020-12-04 14:23:12,877] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:12,902] INFO [RequestSendThread controllerId=1] Controller 1 connected to broker:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[36mbroker                       |[0m [2020-12-04 14:23:12,925] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[35mhive-metastore               |[0m 2020-12-04T14:23:12,948 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Starting hive metastore on port 9083
[36mbroker                       |[0m [2020-12-04 14:23:12,983] TRACE [Controller id=1 epoch=6] Changed state of replica 1 for partition __confluent.support.metrics-0 from OnlineReplica to OnlineReplica (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:12,985] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mbroker                       |[0m [2020-12-04 14:23:12,995] TRACE [Controller id=1 epoch=6] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__confluent.support.metrics', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=false) to broker 1 for partition __confluent.support.metrics-0 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,000] TRACE [Controller id=1 epoch=6] Sending UpdateMetadata request UpdateMetadataPartitionState(topicName='__confluent.support.metrics', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(1) for partition __confluent.support.metrics-0 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,001] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:13,003] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> Map([Topic=__confluent.support.metrics,Partition=0,Replica=1] -> OnlineReplica) (kafka.controller.ZkReplicaStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:13,006] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:13,013] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:13,021] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> Map(__confluent.support.metrics-0 -> OnlinePartition) (kafka.controller.ZkPartitionStateMachine)
[36mbroker                       |[0m [2020-12-04 14:23:13,029] INFO [Controller id=1] Ready to serve as the new controller with epoch 6 (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,132] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,134] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,135] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,136] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,140] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,165] INFO [SocketServer brokerId=1] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[36mbroker                       |[0m [2020-12-04 14:23:13,215] INFO Kafka version: 5.5.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,215] INFO Kafka commitId: 3c4783aac9e33249 (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,215] INFO Kafka startTimeMs: 1607091793178 (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,230] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[36mbroker                       |[0m [2020-12-04 14:23:13,232] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:13,306] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[36mbroker                       |[0m [2020-12-04 14:23:13,310] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[36mbroker                       |[0m [2020-12-04 14:23:13,312] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
[35mhive-metastore               |[0m 2020-12-04T14:23:13,382 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[36mbroker                       |[0m [2020-12-04 14:23:13,456] TRACE [Controller id=1 epoch=6] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker broker:9092 (id: 1 rack: null) (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,485] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__confluent.support.metrics', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=false) correlation id 1 from controller 1 epoch 6 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,543] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 6 starting the become-leader transition for partition __confluent.support.metrics-0 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,548] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__confluent.support.metrics-0) (kafka.server.ReplicaFetcherManager)
[36mbroker                       |[0m [2020-12-04 14:23:13,578] INFO [Partition __confluent.support.metrics-0 broker=1] Log loaded for partition __confluent.support.metrics-0 with initial high watermark 5 (kafka.cluster.Partition)
[36mbroker                       |[0m [2020-12-04 14:23:13,590] INFO [Partition __confluent.support.metrics-0 broker=1] __confluent.support.metrics-0 starts at leader epoch 0 from offset 5 with high watermark 5. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mbroker                       |[0m [2020-12-04 14:23:13,616] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[36mbroker                       |[0m [2020-12-04 14:23:13,617] TRACE [Broker id=1] Stopped fetchers as part of become-leader request from controller 1 epoch 6 with correlation id 1 for partition __confluent.support.metrics-0 (last update controller epoch 1) (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,638] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 6 for the become-leader transition for partition __confluent.support.metrics-0 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,656] INFO ProducerConfig values: 
[36mbroker                       |[0m 	acks = 1
[36mbroker                       |[0m 	batch.size = 16384
[36mbroker                       |[0m 	bootstrap.servers = [PLAINTEXT://broker:9092]
[36mbroker                       |[0m 	buffer.memory = 33554432
[36mbroker                       |[0m 	client.dns.lookup = default
[36mbroker                       |[0m 	client.id = producer-1
[36mbroker                       |[0m 	compression.type = none
[36mbroker                       |[0m 	connections.max.idle.ms = 540000
[36mbroker                       |[0m 	delivery.timeout.ms = 120000
[36mbroker                       |[0m 	enable.idempotence = false
[36mbroker                       |[0m 	interceptor.classes = []
[36mbroker                       |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mbroker                       |[0m 	linger.ms = 0
[36mbroker                       |[0m 	max.block.ms = 10000
[36mbroker                       |[0m 	max.in.flight.requests.per.connection = 5
[36mbroker                       |[0m 	max.request.size = 1048576
[36mbroker                       |[0m 	metadata.max.age.ms = 300000
[36mbroker                       |[0m 	metadata.max.idle.ms = 300000
[36mbroker                       |[0m 	metric.reporters = []
[36mbroker                       |[0m 	metrics.num.samples = 2
[36mbroker                       |[0m 	metrics.recording.level = INFO
[36mbroker                       |[0m 	metrics.sample.window.ms = 30000
[36mbroker                       |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mbroker                       |[0m 	receive.buffer.bytes = 32768
[36mbroker                       |[0m 	reconnect.backoff.max.ms = 1000
[36mbroker                       |[0m 	reconnect.backoff.ms = 50
[36mbroker                       |[0m 	request.timeout.ms = 30000
[36mbroker                       |[0m 	retries = 2147483647
[36mbroker                       |[0m 	retry.backoff.ms = 100
[36mbroker                       |[0m 	sasl.client.callback.handler.class = null
[36mbroker                       |[0m 	sasl.jaas.config = null
[36mbroker                       |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mbroker                       |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mbroker                       |[0m 	sasl.kerberos.service.name = null
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mbroker                       |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.callback.handler.class = null
[36mbroker                       |[0m 	sasl.login.class = null
[36mbroker                       |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mbroker                       |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mbroker                       |[0m 	sasl.login.refresh.window.factor = 0.8
[36mbroker                       |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mbroker                       |[0m 	sasl.mechanism = GSSAPI
[36mbroker                       |[0m 	security.protocol = PLAINTEXT
[36mbroker                       |[0m 	security.providers = null
[36mbroker                       |[0m 	send.buffer.bytes = 131072
[36mbroker                       |[0m 	ssl.cipher.suites = null
[36mbroker                       |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mbroker                       |[0m 	ssl.endpoint.identification.algorithm = https
[36mbroker                       |[0m 	ssl.key.password = null
[36mbroker                       |[0m 	ssl.keymanager.algorithm = SunX509
[36mbroker                       |[0m 	ssl.keystore.location = null
[36mbroker                       |[0m 	ssl.keystore.password = null
[36mbroker                       |[0m 	ssl.keystore.type = JKS
[36mbroker                       |[0m 	ssl.protocol = TLS
[36mbroker                       |[0m 	ssl.provider = null
[36mbroker                       |[0m 	ssl.secure.random.implementation = null
[36mbroker                       |[0m 	ssl.trustmanager.algorithm = PKIX
[36mbroker                       |[0m 	ssl.truststore.location = null
[36mbroker                       |[0m 	ssl.truststore.password = null
[36mbroker                       |[0m 	ssl.truststore.type = JKS
[36mbroker                       |[0m 	transaction.timeout.ms = 60000
[36mbroker                       |[0m 	transactional.id = null
[36mbroker                       |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mbroker                       |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mbroker                       |[0m [2020-12-04 14:23:13,682] TRACE [Controller id=1 epoch=6] Received response {error_code=0,partition_errors=[{topic_name=__confluent.support.metrics,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker broker:9092 (id: 1 rack: null) (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,699] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__confluent.support.metrics', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __confluent.support.metrics-0 in response to UpdateMetadata request sent by controller 1 epoch 6 with correlation id 2 (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,702] TRACE [Controller id=1 epoch=6] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker broker:9092 (id: 1 rack: null) (state.change.logger)
[36mbroker                       |[0m [2020-12-04 14:23:13,729] INFO Kafka version: 5.5.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,731] INFO Kafka commitId: 3c4783aac9e33249 (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,731] INFO Kafka startTimeMs: 1607091793721 (org.apache.kafka.common.utils.AppInfoParser)
[36mbroker                       |[0m [2020-12-04 14:23:13,840] INFO [Producer clientId=producer-1] Cluster ID: 1PGv1UMSTLGPjgLbzLo1-g (org.apache.kafka.clients.Metadata)
[36mbroker                       |[0m [2020-12-04 14:23:14,102] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mbroker                       |[0m [2020-12-04 14:23:14,116] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[36mbroker                       |[0m [2020-12-04 14:23:16,686] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[34mhive-server                  |[0m [6/100] check for hive-metastore:9083...
[34mhive-server                  |[0m [6/100] hive-metastore:9083 is not available yet
[34mhive-server                  |[0m [6/100] try in 5s once again ...
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:18,009 Gossiper.java:1812 - No gossip backlog; proceeding
[33;1mpyspark-app                  |[0m Traceback (most recent call last):
[33;1mpyspark-app                  |[0m   File "src/pipeline.py", line 58, in <module>
[33;1mpyspark-app                  |[0m     run()
[33;1mpyspark-app                  |[0m   File "src/pipeline.py", line 44, in run
[33;1mpyspark-app                  |[0m     sink = L(df)
[33;1mpyspark-app                  |[0m   File "/usr/src/app/src/load.py", line 17, in loader
[33;1mpyspark-app                  |[0m     create_table(features_set_df, keyspace, table_name, primary_key)
[33;1mpyspark-app                  |[0m   File "/usr/src/app/src/connectors/cassandra.py", line 33, in create_table
[33;1mpyspark-app                  |[0m     session = cluster.connect()
[33;1mpyspark-app                  |[0m   File "cassandra/cluster.py", line 1667, in cassandra.cluster.Cluster.connect
[33;1mpyspark-app                  |[0m   File "cassandra/cluster.py", line 1703, in cassandra.cluster.Cluster.connect
[33;1mpyspark-app                  |[0m   File "cassandra/cluster.py", line 1690, in cassandra.cluster.Cluster.connect
[33;1mpyspark-app                  |[0m   File "cassandra/cluster.py", line 3488, in cassandra.cluster.ControlConnection.connect
[33;1mpyspark-app                  |[0m   File "cassandra/cluster.py", line 3533, in cassandra.cluster.ControlConnection._reconnect_internal
[33;1mpyspark-app                  |[0m cassandra.cluster.NoHostAvailable: ('Unable to connect to any servers', {'172.21.0.5:9042': ConnectionRefusedError(111, "Tried connecting to [('172.21.0.5', 9042)]. Last error: Connection refused")})
[36mbroker                       |[0m [2020-12-04 14:23:18,236] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:18,237] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:18,243] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:23:18,245] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[35mhive-metastore               |[0m 2020-12-04T14:23:18,286 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
[35mhive-metastore               |[0m 2020-12-04T14:23:18,291 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
[35mhive-metastore               |[0m 2020-12-04T14:23:18,323 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:18,432 NativeTransportService.java:68 - Netty using native Epoll event loop
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:18,544 Server.java:158 - Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:18,545 Server.java:159 - Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
[35mhive-metastore               |[0m 2020-12-04T14:23:18,580 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Starting DB backed MetaStore Server with SetUGI enabled
[35mhive-metastore               |[0m 2020-12-04T14:23:18,592 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Started the new metaserver on port [9083]...
[35mhive-metastore               |[0m 2020-12-04T14:23:18,592 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Options.minWorkerThreads = 200
[35mhive-metastore               |[0m 2020-12-04T14:23:18,592 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Options.maxWorkerThreads = 1000
[35mhive-metastore               |[0m 2020-12-04T14:23:18,592 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStore - TCP keepalive = true
[32mfeature_store_cassandra      |[0m INFO  [main] 2020-12-04 14:23:18,631 CassandraDaemon.java:557 - Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
[33;1mpyspark-app exited with code 1
[0m[34mhive-server                  |[0m [7/100] hive-metastore:9083 is available.
[34mhive-server                  |[0m mkdir: `/tmp': File exists
[34mhive-server                  |[0m 2020-12-04 14:23:29: Starting HiveServer2
[34mhive-server                  |[0m SLF4J: Class path contains multiple SLF4J bindings.
[34mhive-server                  |[0m SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[34mhive-server                  |[0m SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[34mhive-server                  |[0m SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
[34mhive-server                  |[0m SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
[35mhive-metastore               |[0m 2020-12-04T14:23:34,972 INFO [pool-7-thread-2] org.apache.hadoop.hive.metastore.HiveMetaStore - 1: source:172.21.0.9 get_all_functions
[35mhive-metastore               |[0m 2020-12-04T14:23:35,062 INFO [pool-7-thread-2] org.apache.hadoop.hive.metastore.HiveMetaStore - 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
[35mhive-metastore               |[0m 2020-12-04T14:23:35,197 INFO [pool-7-thread-2] org.apache.hadoop.hive.metastore.HiveMetaStore - 1: source:172.21.0.9 get_all_databases
[35mhive-metastore               |[0m 2020-12-04T14:23:35,229 INFO [pool-7-thread-2] org.apache.hadoop.hive.metastore.HiveMetaStore - 1: source:172.21.0.9 get_all_tables: db=default
[35mhive-metastore               |[0m 2020-12-04T14:23:35,284 INFO [pool-7-thread-2] org.apache.hadoop.hive.metastore.HiveMetaStore - 1: source:172.21.0.9 get_multi_table : db=default tbls=
[36mbroker                       |[0m [2020-12-04 14:28:18,246] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:28:18,247] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:28:18,249] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:28:18,249] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:33:12,602] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mbroker                       |[0m [2020-12-04 14:33:18,250] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:33:18,250] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:33:18,252] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:33:18,252] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:38:18,253] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:38:18,253] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:38:18,254] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:38:18,255] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[33mdatanode                     |[0m 20/12/04 14:42:27 INFO datanode.DirectoryScanner: BlockPool BP-949235150-172.21.0.8-1607088751745 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
[36mbroker                       |[0m [2020-12-04 14:43:12,601] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mbroker                       |[0m [2020-12-04 14:43:18,256] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:43:18,256] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:43:18,257] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[36mbroker                       |[0m [2020-12-04 14:43:18,257] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
Stopping broker                                                ... 
Stopping hive-server                                           ... 
Stopping namenode                                              ... 
Stopping zookeeper                                             ... 
Stopping hive-metastore                                        ... 
Stopping feature_store_cassandra                               ... 
Stopping datanode                                              ... 
Stopping spark-based-feature-store_hive-metastore-postgresql_1 ... 
Killing broker                                                 ... 
Killing hive-server                                            ... 
Killing namenode                                               ... 
Killing zookeeper                                              ... 
Killing hive-metastore                                         ... 
Killing feature_store_cassandra                                ... 
Killing datanode                                               ... 
Killing spark-based-feature-store_hive-metastore-postgresql_1  ... 
[5A[2KKilling zookeeper                                              ... [32mdone[0m[5B[1A[2KKilling spark-based-feature-store_hive-metastore-postgresql_1  ... [32mdone[0m[1B[4A[2KKilling hive-metastore                                         ... [32mdone[0m[4B[6A[2KKilling namenode                                               ... [32mdone[0m[6B[7A[2KKilling hive-server                                            ... [32mdone[0m[7B[2A[2KKilling datanode                                               ... [32mdone[0m[2B[8A[2KKilling broker                                                 ... [32mdone[0m[8B[3A[2KKilling feature_store_cassandra                                ... [32mdone[0m[3BGracefully stopping... (press Ctrl+C again to force)
